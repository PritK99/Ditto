"""
This script extracts C / C++ codes from raw jsons of transcodeocean dataset.
"""
import os
import random
import re
import hashlib

def clear_output_files():
    """
    Clears the content of all output files (/data)
    """

    files = [
        'data/unpaired_c.txt',
    ]

    for file_path in files:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        # Open and immediately close to clear the file
        open(file_path, 'w', encoding='utf-8').close()
    
    print("Output files cleared.")

def remove_comments(code: str) -> str:
    """
    Remove C/C++ comments from code.
    """
    code_no_single = re.sub(r'//.*', '', code)
    code_no_comments = re.sub(r'/\*.*?\*/', '', code_no_single, flags=re.DOTALL)
    return code_no_comments

def escape_code(code: str) -> str:
    """
    Escape newlines and tabs.
    """

    return code.replace("\\", "\\\\").replace("\n", "\\n").replace("\t", "\\t")

def create_dataset(folder_path: str, n: int, output_file: str):
    """ 
    Processing n codefiles from raw_data directory
    """

    all_files = [f for f in os.listdir(folder_path) if f.endswith((".c", ".cpp"))]
    chosen_files = random.sample(all_files, min(n, len(all_files)))    # We decide to sample files as the files are generated by different models

    seen_hashes = set()
    processed_count = 0
    unique_count = 0

    with open(output_file, "w", encoding="utf-8") as out_f:
        for filename in chosen_files:
            file_path = os.path.join(folder_path, filename)
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                code = f.read()
                code_no_comments = remove_comments(code)
                escaped = escape_code(code_no_comments)

                # Compute hash to avoid duplicates as we can not store all files in string format in set
                code_hash = hashlib.sha256(escaped.encode('utf-8')).hexdigest()

                if code_hash not in seen_hashes:
                    seen_hashes.add(code_hash)
                    out_f.write(escaped + "\n")
                    unique_count += 1

                processed_count += 1
                if processed_count % 10000 == 0:
                    print(f"Processed {processed_count} files, {unique_count} unique codes saved.")

    print(f"Included {unique_count} unique files out of {processed_count} processed.")

raw_dataset_path = "./raw_data"
num_files = 120000    # We take more files than required as these will get further proccessed before appending to final_data directory
output_file = "./data/unpaired_c.txt"
create_dataset(raw_dataset_path, num_files, output_file)

# clear_output_files()
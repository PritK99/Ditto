{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EzRlaL4XjqtH",
        "yxdhssnejvWb",
        "ur7P3Epdjzmi",
        "3D0sxREbi_g-"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D9edcHV-Lja",
        "outputId": "af6d2c61-61db-4b40-924e-7382b49aa885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.1\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.19.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import ast\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import random"
      ],
      "metadata": {
        "id": "Yv1Q78nV5RFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "EzRlaL4XjqtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Operators and punctuation (multi-char first)\n",
        "MULTI_CHAR_OPS = [\n",
        "    '==', '!=', '<=', '>=', '++', '--', '+=', '-=', '*=', '/=', '%=', '&&', '||', '<<', '>>', '->', '::', '...'\n",
        "]\n",
        "# Single char symbols\n",
        "SINGLE_CHAR_SYMS = list(\"#{}()[].,;<>+-*/=%&|^~!:?\")\n",
        "\n",
        "# Build regex pattern\n",
        "# 1. String literals\n",
        "# 2. Multi-char ops\n",
        "# 3. Single-char symbols\n",
        "# 4. Identifiers\n",
        "# 5. Numbers\n",
        "TOKEN_PATTERN = re.compile(\n",
        "    r'''\n",
        "    \"(?:[^\"\\\\]|\\\\.)*\"             |   # double-quoted string literals\n",
        "    '(?:[^'\\\\]|\\\\.)*'             |   # single-quoted char literals\n",
        "    ''' + '|'.join(re.escape(op) for op in MULTI_CHAR_OPS) + r''' |   # multi-char operators\n",
        "    [''' + re.escape(''.join(SINGLE_CHAR_SYMS)) + r'''] |             # single-char symbols\n",
        "    [A-Za-z_]\\w*                  |   # identifiers\n",
        "    \\d+(?:\\.\\d+)?                     # numbers\n",
        "    ''', re.VERBOSE\n",
        ")\n",
        "\n",
        "# Regex to remove comments\n",
        "LINE_COMMENT = re.compile(r'//.*')\n",
        "BLOCK_COMMENT = re.compile(r'/\\*.*?\\*/', re.DOTALL)\n",
        "\n",
        "def tokenize_code_line(line: str):\n",
        "    # Remove comments\n",
        "    line = BLOCK_COMMENT.sub('', line)\n",
        "    line = LINE_COMMENT.sub('', line)\n",
        "\n",
        "    # Replace escaped newlines/tabs with space\n",
        "    line = line.replace(\"\\\\n\", \" \").replace(\"\\\\t\", \" \").replace(\"\\\\r\", \" \")\n",
        "\n",
        "    # Collapse multiple spaces\n",
        "    line = re.sub(r'\\s+', ' ', line).strip()\n",
        "    if not line:\n",
        "        return []\n",
        "\n",
        "    # Find tokens\n",
        "    tokens = [m.group(0) for m in TOKEN_PATTERN.finditer(line)]\n",
        "    return tokens\n",
        "\n",
        "def process_file(input_path: Path, output_path: Path):\n",
        "    tokenized_lines = []\n",
        "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            tokens = tokenize_code_line(line)\n",
        "            if tokens:\n",
        "                tokenized_lines.append(tokens)\n",
        "\n",
        "    with output_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for tokens in tokenized_lines:\n",
        "            f.write(json.dumps(tokens) + \"\\n\")\n",
        "\n",
        "    return tokenized_lines\n",
        "\n",
        "def split_and_save(token_lists, base_name: str, train_ratio=0.8, val_ratio=0.1):\n",
        "    random.shuffle(token_lists)\n",
        "    n = len(token_lists)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(n * (train_ratio + val_ratio))\n",
        "    splits = {\n",
        "        'train': token_lists[:train_end],\n",
        "        'val': token_lists[train_end:val_end],\n",
        "        'test': token_lists[val_end:]\n",
        "    }\n",
        "\n",
        "    for split_name, data in splits.items():\n",
        "        path = f\"{base_name}_{split_name}.txt\"\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            for tokens in data:\n",
        "                f.write(json.dumps(tokens) + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Input files\n",
        "    c_tokens = process_file(Path(\"unpaired_c.txt\"), Path(\"c_processed.txt\"))\n",
        "    cpp_tokens = process_file(Path(\"unpaired_c++.txt\"), Path(\"cpp_processed.txt\"))\n",
        "\n",
        "    # Create train/val/test splits\n",
        "    split_and_save(c_tokens, \"c_code\")\n",
        "    split_and_save(cpp_tokens, \"cpp_code\")\n",
        "\n",
        "    print(\"✅ Done! Tokenized long code lines correctly and created train/val/test splits.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gyYKSqnIYW9",
        "outputId": "4229e9e1-372a-4aaf-ac49-f573b6c0bb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! Tokenized long code lines correctly and created train/val/test splits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "yxdhssnejvWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.text import BLEUScore\n",
        "from jiwer import wer, cer\n",
        "\n",
        "class MetricsCalculator:\n",
        "    \"\"\"\n",
        "    Calculate BLEU, CER, and WER metrics for sequence reconstruction.\n",
        "    \"\"\"\n",
        "    def __init__(self, idx_to_token: Dict[int, str], pad_idx: int = 0):\n",
        "        self.idx_to_token = idx_to_token\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        # Initialize BLEU calculator\n",
        "        self.bleu_calculator = BLEUScore(n_gram=4)\n",
        "\n",
        "    def indices_to_tokens(self, indices: torch.Tensor) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Convert token indices to list of token strings.\n",
        "\n",
        "        Args:\n",
        "            indices: (batch_size, seq_len)\n",
        "\n",
        "        Returns:\n",
        "            List of token lists for each sequence\n",
        "        \"\"\"\n",
        "        batch_sequences = []\n",
        "        for seq in indices:\n",
        "            tokens = []\n",
        "            for idx in seq:\n",
        "                idx_val = idx.item()\n",
        "                if idx_val != self.pad_idx:  # Skip padding\n",
        "                    tokens.append(self.idx_to_token.get(idx_val, '<UNK>'))\n",
        "            batch_sequences.append(tokens)\n",
        "        return batch_sequences\n",
        "\n",
        "    def tokens_to_string(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Join tokens into a single string.\"\"\"\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def calculate_bleu(self, predictions: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Calculate BLEU score.\n",
        "\n",
        "        Args:\n",
        "            predictions: (batch_size, seq_len) - predicted token indices\n",
        "            targets: (batch_size, seq_len) - target token indices\n",
        "\n",
        "        Returns:\n",
        "            BLEU score (0-1)\n",
        "        \"\"\"\n",
        "        # Convert indices to tokens\n",
        "        pred_tokens = self.indices_to_tokens(predictions)\n",
        "        target_tokens = self.indices_to_tokens(targets)\n",
        "\n",
        "        # Convert to strings for BLEU calculation\n",
        "        pred_strings = [self.tokens_to_string(tokens) for tokens in pred_tokens]\n",
        "        target_strings = [[self.tokens_to_string(tokens)] for tokens in target_tokens]\n",
        "\n",
        "        # Calculate BLEU\n",
        "        try:\n",
        "            bleu_score = self.bleu_calculator(pred_strings, target_strings)\n",
        "            return bleu_score.item()\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_cer(self, predictions: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Character Error Rate (CER).\n",
        "\n",
        "        Args:\n",
        "            predictions: (batch_size, seq_len) - predicted token indices\n",
        "            targets: (batch_size, seq_len) - target token indices\n",
        "\n",
        "        Returns:\n",
        "            CER score (lower is better, 0 is perfect)\n",
        "        \"\"\"\n",
        "        # Convert indices to tokens\n",
        "        pred_tokens = self.indices_to_tokens(predictions)\n",
        "        target_tokens = self.indices_to_tokens(targets)\n",
        "\n",
        "        # Convert to strings\n",
        "        pred_strings = [self.tokens_to_string(tokens) for tokens in pred_tokens]\n",
        "        target_strings = [self.tokens_to_string(tokens) for tokens in target_tokens]\n",
        "\n",
        "        # Calculate CER for each pair and average\n",
        "        try:\n",
        "            cer_scores = []\n",
        "            for pred, target in zip(pred_strings, target_strings):\n",
        "                if len(target) > 0:  # Avoid empty references\n",
        "                    cer_score = cer(target, pred)\n",
        "                    cer_scores.append(cer_score)\n",
        "\n",
        "            return np.mean(cer_scores) if cer_scores else 0.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_wer(self, predictions: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Word Error Rate (WER).\n",
        "\n",
        "        Args:\n",
        "            predictions: (batch_size, seq_len) - predicted token indices\n",
        "            targets: (batch_size, seq_len) - target token indices\n",
        "\n",
        "        Returns:\n",
        "            WER score (lower is better, 0 is perfect)\n",
        "        \"\"\"\n",
        "        # Convert indices to tokens\n",
        "        pred_tokens = self.indices_to_tokens(predictions)\n",
        "        target_tokens = self.indices_to_tokens(targets)\n",
        "\n",
        "        # Convert to strings\n",
        "        pred_strings = [self.tokens_to_string(tokens) for tokens in pred_tokens]\n",
        "        target_strings = [self.tokens_to_string(tokens) for tokens in target_tokens]\n",
        "\n",
        "        # Calculate WER for each pair and average\n",
        "        try:\n",
        "            wer_scores = []\n",
        "            for pred, target in zip(pred_strings, target_strings):\n",
        "                if len(target.split()) > 0:  # Avoid empty references\n",
        "                    wer_score = wer(target, pred)\n",
        "                    wer_scores.append(wer_score)\n",
        "\n",
        "            return np.mean(wer_scores) if wer_scores else 0.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_all_metrics(self, predictions: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate all metrics (BLEU, CER, WER).\n",
        "\n",
        "        Args:\n",
        "            predictions: (batch_size, seq_len) - predicted token indices\n",
        "            targets: (batch_size, seq_len) - target token indices\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all metrics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'bleu': self.calculate_bleu(predictions, targets),\n",
        "            'cer': self.calculate_cer(predictions, targets),\n",
        "            'wer': self.calculate_wer(predictions, targets)\n",
        "        }"
      ],
      "metadata": {
        "id": "UhreQTG59_No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "ur7P3Epdjzmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Transformer Encoder using PyTorch's built-in TransformerEncoder.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6,\n",
        "                 dim_feedforward=2048, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Using PyTorch's built-in TransformerEncoderLayer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Stack multiple encoder layers\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: (batch_size, seq_len)\n",
        "            src_mask: (seq_len, seq_len) - attention mask\n",
        "            src_key_padding_mask: (batch_size, seq_len) - padding mask\n",
        "        \"\"\"\n",
        "        # Embedding with scaling\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        src = src.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        src = src.transpose(0, 1)  # Back to (batch_size, seq_len, d_model)\n",
        "        src = self.dropout(src)\n",
        "\n",
        "        # Pass through transformer encoder\n",
        "        output = self.transformer_encoder(\n",
        "            src,\n",
        "            mask=src_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        return output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Transformer Decoder using PyTorch's built-in TransformerDecoder.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Using PyTorch's built-in TransformerDecoderLayer\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Stack multiple decoder layers\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tgt: (batch_size, tgt_seq_len)\n",
        "            memory: (batch_size, src_seq_len, d_model) - encoder output\n",
        "            tgt_mask: (tgt_seq_len, tgt_seq_len) - causal mask\n",
        "            memory_mask: (tgt_seq_len, src_seq_len) - cross attention mask\n",
        "            tgt_key_padding_mask: (batch_size, tgt_seq_len) - target padding mask\n",
        "            memory_key_padding_mask: (batch_size, src_seq_len) - source padding mask\n",
        "        \"\"\"\n",
        "        # Embedding with scaling\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        tgt = tgt.transpose(0, 1)  # (seq_len, batch_size, d_model)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        tgt = tgt.transpose(0, 1)  # Back to (batch_size, seq_len, d_model)\n",
        "        tgt = self.dropout(tgt)\n",
        "\n",
        "        # Pass through transformer decoder\n",
        "        output = self.transformer_decoder(\n",
        "            tgt=tgt,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=memory_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        output = self.output_projection(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8,\n",
        "                 num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048,\n",
        "                 dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size=src_vocab_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_encoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            max_len=max_len\n",
        "        )\n",
        "\n",
        "        self.decoder1 = Decoder(\n",
        "            vocab_size=tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            max_len=max_len\n",
        "        )\n",
        "\n",
        "\n",
        "        self.decoder2 = Decoder(\n",
        "            vocab_size=tgt_vocab_size,\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            max_len=max_len\n",
        "        )\n",
        "\n",
        "    def forward(self, src, tgt, flag, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "\n",
        "        # Encode source sequence\n",
        "        memory = self.encoder(\n",
        "            src=src,\n",
        "            src_mask=src_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        if flag:    # for switching between 2 languages\n",
        "            output = self.decoder1(\n",
        "                tgt=tgt,\n",
        "                memory=memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            output = self.decoder2(\n",
        "                tgt=tgt,\n",
        "                memory=memory,\n",
        "                tgt_mask=tgt_mask,\n",
        "                memory_mask=memory_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "S5ieA9PbzveT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1a-4WYAzqtU"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for transformer inputs.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility functions for creating masks\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"Generate a square mask for the sequence to prevent attention to future positions.\"\"\"\n",
        "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "ruxbPWchztmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loaders"
      ],
      "metadata": {
        "id": "rNZ7xYmpj3CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import ast\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import random\n",
        "\n",
        "class TokenizedCodeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file_path: str, max_seq_len: int = 512, min_seq_len: int = 5,\n",
        "                 vocab_size: int = None, test_mode: bool = False, vocab: List[str] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            file_path: Path to tokenized text file\n",
        "            max_seq_len: Maximum sequence length (longer sequences will be truncated)\n",
        "            min_seq_len: Minimum sequence length (shorter sequences will be filtered)\n",
        "            vocab_size: Maximum vocabulary size (None for no limit)\n",
        "            test_mode: If True, loads only first 1000 lines for testing\n",
        "        \"\"\"\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.min_seq_len = min_seq_len\n",
        "\n",
        "        # Special tokens\n",
        "        self.PAD_TOKEN = '<PAD>'\n",
        "        self.UNK_TOKEN = '<UNK>'\n",
        "        self.SOS_TOKEN = '<SOS>'  # Start of sequence\n",
        "        self.EOS_TOKEN = '<EOS>'  # End of sequence\n",
        "\n",
        "        # Load and process data\n",
        "        self.sequences = self._load_data(file_path, test_mode)\n",
        "        if vocab is None:\n",
        "            # Build vocab from this dataset\n",
        "            self.vocab = self._build_vocab(vocab_size)\n",
        "        else:\n",
        "            # Use given vocab (shared across datasets)\n",
        "            self.vocab = vocab\n",
        "        self.token_to_idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
        "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "        # Convert sequences to indices\n",
        "        self.encoded_sequences = self._encode_sequences()\n",
        "\n",
        "        print(f\"Loaded {len(self.encoded_sequences)} sequences\")\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "        print(f\"Average sequence length: {sum(len(seq) for seq in self.sequences) / len(self.sequences):.1f}\")\n",
        "\n",
        "    def _load_data(self, file_path: str, test_mode: bool) -> List[List[str]]:\n",
        "        \"\"\"Load tokenized sequences from file where each line looks like [\"#\", \"include\", \"<\"].\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if test_mode and i >= 1000:  # Limit for testing\n",
        "                    break\n",
        "\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Parse the list of tokens from string representation\n",
        "                    tokens = ast.literal_eval(line)\n",
        "\n",
        "                    # Ensure it's a list of strings\n",
        "                    if isinstance(tokens, list) and len(tokens) >= self.min_seq_len:\n",
        "                        # Convert everything to string (in case of numbers or symbols)\n",
        "                        tokens = [str(token) for token in tokens]\n",
        "\n",
        "                        # Truncate if needed (reserve space for SOS/EOS)\n",
        "                        if len(tokens) > self.max_seq_len - 2:\n",
        "                            tokens = tokens[: self.max_seq_len - 2]\n",
        "\n",
        "                        sequences.append(tokens)\n",
        "\n",
        "                except (ValueError, SyntaxError) as e:\n",
        "                    print(f\"Skipping line {i+1}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if not sequences:\n",
        "            raise ValueError(\"No valid sequences found in the file!\")\n",
        "\n",
        "        return sequences\n",
        "\n",
        "\n",
        "    def _build_vocab(self, vocab_size: int = None, min_freq: int = 3) -> List[str]:\n",
        "        \"\"\"Build vocabulary from all tokens with frequency threshold.\"\"\"\n",
        "        # Count all tokens\n",
        "        token_counts = Counter()\n",
        "        for sequence in self.sequences:\n",
        "            token_counts.update(sequence)\n",
        "\n",
        "        # Start with special tokens\n",
        "        vocab = [self.PAD_TOKEN, self.UNK_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN]\n",
        "\n",
        "        # Filter tokens by frequency (only keep tokens with count >= min_freq)\n",
        "        filtered_tokens = [(tok, cnt) for tok, cnt in token_counts.items() if cnt >= min_freq]\n",
        "\n",
        "        # Sort by frequency (most common first)\n",
        "        filtered_tokens.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Apply vocab size limit if given\n",
        "        if vocab_size is not None:\n",
        "            remaining_vocab_size = vocab_size - len(vocab)\n",
        "            filtered_tokens = filtered_tokens[:remaining_vocab_size]\n",
        "\n",
        "        # Add to vocab\n",
        "        vocab.extend([tok for tok, _ in filtered_tokens])\n",
        "\n",
        "        return vocab\n",
        "\n",
        "    def _encode_sequences(self) -> List[List[int]]:\n",
        "        \"\"\"Convert token sequences to index sequences, replace rare tokens with <UNK>.\"\"\"\n",
        "        encoded = []\n",
        "        unk_idx = self.token_to_idx[self.UNK_TOKEN]\n",
        "        sos_idx = self.token_to_idx[self.SOS_TOKEN]\n",
        "        eos_idx = self.token_to_idx[self.EOS_TOKEN]\n",
        "\n",
        "        for sequence in self.sequences:\n",
        "            # Add SOS and EOS tokens\n",
        "            full_sequence = [self.SOS_TOKEN] + sequence + [self.EOS_TOKEN]\n",
        "\n",
        "            # Convert to indices, replace unknown tokens with <UNK>\n",
        "            indices = []\n",
        "            for token in full_sequence:\n",
        "                indices.append(self.token_to_idx.get(token, unk_idx))\n",
        "\n",
        "            encoded.append(indices)\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encoded_sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Return source and target sequences for reconstruction.\n",
        "        For unsupervised learning, target is the same as source but shifted.\n",
        "        \"\"\"\n",
        "        sequence = self.encoded_sequences[idx]\n",
        "\n",
        "        # For encoder-decoder reconstruction:\n",
        "        # Source: full sequence with SOS and EOS\n",
        "        # Target input: sequence without EOS (for teacher forcing)\n",
        "        # Target output: sequence without SOS (for loss calculation)\n",
        "\n",
        "        src_seq = torch.tensor(sequence, dtype=torch.long)\n",
        "        tgt_input = torch.tensor(sequence[:-1], dtype=torch.long)  # Remove EOS\n",
        "        tgt_output = torch.tensor(sequence[1:], dtype=torch.long)  # Remove SOS\n",
        "\n",
        "        return {\n",
        "            'src': src_seq,           # Input to encoder\n",
        "            'tgt_input': tgt_input,   # Input to decoder (with teacher forcing)\n",
        "            'tgt_output': tgt_output, # Target for loss calculation\n",
        "            'src_len': len(src_seq),\n",
        "            'tgt_len': len(tgt_input)\n",
        "        }\n",
        "\n",
        "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Collate function to pad sequences in a batch.\n",
        "    \"\"\"\n",
        "    # Separate different components\n",
        "    src_seqs = [item['src'] for item in batch]\n",
        "    tgt_input_seqs = [item['tgt_input'] for item in batch]\n",
        "    tgt_output_seqs = [item['tgt_output'] for item in batch]\n",
        "    src_lens = torch.tensor([item['src_len'] for item in batch])\n",
        "    tgt_lens = torch.tensor([item['tgt_len'] for item in batch])\n",
        "\n",
        "    # Pad sequences\n",
        "    src_padded = pad_sequence(src_seqs, batch_first=True, padding_value=0)  # PAD token is at index 0\n",
        "    tgt_input_padded = pad_sequence(tgt_input_seqs, batch_first=True, padding_value=0)\n",
        "    tgt_output_padded = pad_sequence(tgt_output_seqs, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Create padding masks (True for real tokens, False for padding)\n",
        "    src_mask = (src_padded != 0)\n",
        "    tgt_mask = (tgt_input_padded != 0)\n",
        "\n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt_input': tgt_input_padded,\n",
        "        'tgt_output': tgt_output_padded,\n",
        "        'src_mask': src_mask,\n",
        "        'tgt_mask': tgt_mask,\n",
        "        'src_lens': src_lens,\n",
        "        'tgt_lens': tgt_lens\n",
        "    }\n",
        "\n",
        "def create_dataloader(file_path: str, batch_size: int = 32, shuffle: bool = True,\n",
        "                     max_seq_len: int = 512, min_seq_len: int = 5,\n",
        "                     vocab_size: int = None, test_mode: bool = False,\n",
        "                     num_workers: int = 0, vocab: List[str] = None) -> Tuple[DataLoader, TokenizedCodeDataset]:\n",
        "    \"\"\"\n",
        "    Create a DataLoader for the tokenized code dataset.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the tokenized text file\n",
        "        batch_size: Batch size for training\n",
        "        shuffle: Whether to shuffle the dataset\n",
        "        max_seq_len: Maximum sequence length\n",
        "        min_seq_len: Minimum sequence length\n",
        "        vocab_size: Maximum vocabulary size\n",
        "        test_mode: Load only subset for testing\n",
        "        num_workers: Number of workers for data loading\n",
        "\n",
        "    Returns:\n",
        "        DataLoader and Dataset objects\n",
        "    \"\"\"\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = TokenizedCodeDataset(\n",
        "        file_path=file_path,\n",
        "        max_seq_len=max_seq_len,\n",
        "        min_seq_len=min_seq_len,\n",
        "        vocab_size=vocab_size,\n",
        "        test_mode=test_mode,\n",
        "        vocab = vocab\n",
        "    )\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    return dataloader, dataset"
      ],
      "metadata": {
        "id": "f-nHuHkSOkSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trainer**"
      ],
      "metadata": {
        "id": "3D0sxREbi_g-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import time\n",
        "from typing import Dict, Tuple\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "class ReconstructionLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Reconstruction loss for sequence-to-sequence learning.\n",
        "    Uses CrossEntropyLoss with padding token masking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int, pad_idx: int = 0, label_smoothing: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.pad_idx = pad_idx\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            ignore_index=pad_idx,  # Ignore padding tokens in loss\n",
        "            label_smoothing=label_smoothing\n",
        "        )\n",
        "\n",
        "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor,\n",
        "                target_mask: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Calculate reconstruction loss.\n",
        "\n",
        "        Args:\n",
        "            predictions: (batch_size, seq_len, vocab_size)\n",
        "            targets: (batch_size, seq_len)\n",
        "            target_mask: (batch_size, seq_len) - True for real tokens\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with loss and metrics\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, vocab_size = predictions.shape\n",
        "\n",
        "        # Reshape for CrossEntropyLoss\n",
        "        predictions_flat = predictions.view(-1, vocab_size)  # (batch_size * seq_len, vocab_size)\n",
        "        targets_flat = targets.view(-1)  # (batch_size * seq_len,)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = self.criterion(predictions_flat, targets_flat)\n",
        "\n",
        "        # Calculate accuracy (only for non-padding tokens)\n",
        "        with torch.no_grad():\n",
        "            pred_tokens = torch.argmax(predictions_flat, dim=-1)\n",
        "\n",
        "            if target_mask is not None:\n",
        "                mask_flat = target_mask.view(-1)\n",
        "                correct = ((pred_tokens == targets_flat) & mask_flat).sum().float()\n",
        "                total = mask_flat.sum().float()\n",
        "            else:\n",
        "                # Use ignore_index to exclude padding tokens\n",
        "                valid_mask = targets_flat != self.pad_idx\n",
        "                correct = ((pred_tokens == targets_flat) & valid_mask).sum().float()\n",
        "                total = valid_mask.sum().float()\n",
        "\n",
        "            accuracy = correct / (total + 1e-8)  # Avoid division by zero\n",
        "\n",
        "        # Calculate perplexity\n",
        "        perplexity = torch.exp(loss)\n",
        "\n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'accuracy': accuracy,\n",
        "            'perplexity': perplexity,\n",
        "            'num_tokens': total\n",
        "        }\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Generate causal mask for decoder self-attention.\"\"\"\n",
        "    mask = torch.triu(torch.ones(sz, sz, device=device) * float('-inf'), diagonal=1)\n",
        "    return mask\n",
        "\n",
        "def create_padding_mask(seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:\n",
        "    \"\"\"Create padding mask - True for padding tokens.\"\"\"\n",
        "    return (seq == pad_idx)\n",
        "\n",
        "class TransformerTrainer:\n",
        "    \"\"\"\n",
        "    Trainer class for Transformer reconstruction training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: nn.Module, vocab_size: int, idx_to_token: Dict[int, str], pad_idx: int = 0,\n",
        "                 learning_rate: float = 1e-4, weight_decay: float = 1e-5,\n",
        "                 label_smoothing: float = 0.1, warmup_steps: int = 4000):\n",
        "\n",
        "        self.model = model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = next(model.parameters()).device\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = ReconstructionLoss(vocab_size, pad_idx, label_smoothing)\n",
        "\n",
        "        # Optimizer with learning rate scheduling\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            betas=(0.9, 0.98),\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "        self.metrics_calculator = MetricsCalculator(idx_to_token, pad_idx)\n",
        "\n",
        "        # Learning rate scheduler (Transformer-style warmup)\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.d_model = model.encoder.d_model if hasattr(model.encoder, 'd_model') else 512\n",
        "        self.scheduler = optim.lr_scheduler.LambdaLR(\n",
        "            self.optimizer,\n",
        "            lr_lambda=self._get_lr_scale\n",
        "        )\n",
        "\n",
        "        # Training metrics\n",
        "        self.train_losses_1 = []\n",
        "        self.train_losses_2 = []\n",
        "        self.train_accuracies_1 = []\n",
        "        self.train_accuracies_2 = []\n",
        "        self.val_losses_1 = []\n",
        "        self.val_losses_2 = []\n",
        "        self.val_accuracies_1 = []\n",
        "        self.val_accuracies_2 = []\n",
        "        self.val_bleu_1 = []\n",
        "        self.val_bleu_2 = []\n",
        "        self.val_cer_1 = []\n",
        "        self.val_cer_2 = []\n",
        "        self.val_wer_1 = []\n",
        "        self.val_wer_2 = []\n",
        "        self.step = 0\n",
        "\n",
        "    def _get_lr_scale(self, step: int) -> float:\n",
        "        \"\"\"Learning rate scaling for warmup.\"\"\"\n",
        "        step = max(1, step)  # Avoid division by zero\n",
        "        return (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
        "\n",
        "    def train_step(self, batch: Dict[str, torch.Tensor], flag: bool) -> Dict[str, float]:\n",
        "        \"\"\"Single training step.\"\"\"\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Move batch to device\n",
        "        src = batch['src'].to(self.device)\n",
        "        tgt_input = batch['tgt_input'].to(self.device)\n",
        "        tgt_output = batch['tgt_output'].to(self.device)\n",
        "        tgt_mask_padding = batch['tgt_mask'].to(self.device)\n",
        "\n",
        "        batch_size, tgt_len = tgt_input.shape\n",
        "\n",
        "        # Create masks\n",
        "        src_key_padding_mask = create_padding_mask(src, self.pad_idx)\n",
        "        tgt_key_padding_mask = create_padding_mask(tgt_input, self.pad_idx)\n",
        "        tgt_mask = generate_square_subsequent_mask(tgt_len, self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = self.model(\n",
        "            src=src,\n",
        "            tgt=tgt_input,\n",
        "            flag = flag,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        # Calculate loss\n",
        "        loss_dict = self.criterion(output, tgt_output, tgt_mask_padding)\n",
        "        loss = loss_dict['loss']\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.step += 1\n",
        "\n",
        "\n",
        "\n",
        "        return {\n",
        "            'loss': loss.item(),\n",
        "            'accuracy': loss_dict['accuracy'].item(),\n",
        "            'perplexity': loss_dict['perplexity'].item(),\n",
        "            'lr': self.scheduler.get_last_lr()[0],\n",
        "            'num_tokens': loss_dict['num_tokens'].item()\n",
        "        }\n",
        "\n",
        "    def validate(self, val_loader_1: DataLoader, val_loader_2: DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"Validation loop.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss_1 = 0\n",
        "        total_accuracy_1 = 0\n",
        "        total_tokens_1 = 0\n",
        "        total_bleu_1 = 0\n",
        "        total_cer_1 = 0\n",
        "        total_wer_1 = 0\n",
        "\n",
        "        total_loss_2 = 0\n",
        "        total_accuracy_2 = 0\n",
        "        total_tokens_2 = 0\n",
        "        total_bleu_2 = 0\n",
        "        total_cer_2 = 0\n",
        "        total_wer_2 = 0\n",
        "\n",
        "        num_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_1, batch_2 in tqdm(zip(val_loader_1, val_loader_2), desc=\"Validating\", leave=False):\n",
        "                # Move batch to device\n",
        "                src_1 = batch_1['src'].to(self.device)\n",
        "                tgt_input_1 = batch_1['tgt_input'].to(self.device)\n",
        "                tgt_output_1 = batch_1['tgt_output'].to(self.device)\n",
        "                tgt_mask_padding_1 = batch_1['tgt_mask'].to(self.device)\n",
        "\n",
        "                src_2 = batch_2['src'].to(self.device)\n",
        "                tgt_input_2 = batch_2['tgt_input'].to(self.device)\n",
        "                tgt_output_2 = batch_2['tgt_output'].to(self.device)\n",
        "                tgt_mask_padding_2 = batch_2['tgt_mask'].to(self.device)\n",
        "\n",
        "                batch_size, tgt_len_1 = tgt_input_1.shape\n",
        "                batch_size, tgt_len_2 = tgt_input_2.shape\n",
        "\n",
        "                # Create masks\n",
        "                src_key_padding_mask_1 = create_padding_mask(src_1, self.pad_idx)\n",
        "                tgt_key_padding_mask_1 = create_padding_mask(tgt_input_1, self.pad_idx)\n",
        "                tgt_mask_1 = generate_square_subsequent_mask(tgt_len_1, self.device)\n",
        "\n",
        "                src_key_padding_mask_2 = create_padding_mask(src_2, self.pad_idx)\n",
        "                tgt_key_padding_mask_2 = create_padding_mask(tgt_input_2, self.pad_idx)\n",
        "                tgt_mask_2 = generate_square_subsequent_mask(tgt_len_2, self.device)\n",
        "\n",
        "                # Forward pass\n",
        "                output_1 = self.model(\n",
        "                    src=src_1,\n",
        "                    tgt=tgt_input_1,\n",
        "                    flag=True,\n",
        "                    tgt_mask=tgt_mask_1,\n",
        "                    src_key_padding_mask=src_key_padding_mask_1,\n",
        "                    tgt_key_padding_mask=tgt_key_padding_mask_1,\n",
        "                    memory_key_padding_mask=src_key_padding_mask_1\n",
        "                )\n",
        "\n",
        "                output_2 = self.model(\n",
        "                    src=src_2,\n",
        "                    tgt=tgt_input_2,\n",
        "                    flag=False,\n",
        "                    tgt_mask=tgt_mask_2,\n",
        "                    src_key_padding_mask=src_key_padding_mask_2,\n",
        "                    tgt_key_padding_mask=tgt_key_padding_mask_2,\n",
        "                    memory_key_padding_mask=src_key_padding_mask_2\n",
        "                )\n",
        "\n",
        "                # Calculate loss\n",
        "                loss_dict_1 = self.criterion(output_1, tgt_output_1, tgt_mask_padding_1)\n",
        "                loss_dict_2 = self.criterion(output_2, tgt_output_2, tgt_mask_padding_2)\n",
        "\n",
        "                predictions_1 = torch.argmax(output_1, dim=-1)\n",
        "                predictions_2 = torch.argmax(output_2, dim=-1)\n",
        "\n",
        "                metrics_1 = self.metrics_calculator.calculate_all_metrics(predictions_1, tgt_output_1)\n",
        "                metrics_2 = self.metrics_calculator.calculate_all_metrics(predictions_2, tgt_output_2)\n",
        "\n",
        "                # Accumulate metrics\n",
        "                total_loss_1 += loss_dict_1['loss'].item()\n",
        "                total_accuracy_1 += loss_dict_1['accuracy'].item() * loss_dict_1['num_tokens'].item()\n",
        "                total_tokens_1 += loss_dict_1['num_tokens'].item()\n",
        "                total_bleu_1 += metrics_1['bleu']\n",
        "                total_cer_1 += metrics_1['cer']\n",
        "                total_wer_1 += metrics_1['wer']\n",
        "\n",
        "                total_loss_2 += loss_dict_2['loss'].item()\n",
        "                total_accuracy_2 += loss_dict_2['accuracy'].item() * loss_dict_2['num_tokens'].item()\n",
        "                total_tokens_2 += loss_dict_2['num_tokens'].item()\n",
        "                total_bleu_2 += metrics_2['bleu']\n",
        "                total_cer_2 += metrics_2['cer']\n",
        "                total_wer_2 += metrics_2['wer']\n",
        "\n",
        "                num_batches += 1\n",
        "\n",
        "        avg_loss_1 = total_loss_1 / num_batches\n",
        "        avg_accuracy_1 = total_accuracy_1 / total_tokens_1 if total_tokens_1 > 0 else 0\n",
        "        avg_perplexity_1 = math.exp(avg_loss_1)\n",
        "        avg_bleu_1 = total_bleu_1 / num_batches\n",
        "        avg_cer_1 = total_cer_1 / num_batches\n",
        "        avg_wer_1 = total_wer_1 / num_batches\n",
        "\n",
        "        avg_loss_2 = total_loss_2 / num_batches\n",
        "        avg_accuracy_2 = total_accuracy_2 / total_tokens_2 if total_tokens_2 > 0 else 0\n",
        "        avg_perplexity_2 = math.exp(avg_loss_2)\n",
        "        avg_bleu_2 = total_bleu_2 / num_batches\n",
        "        avg_cer_2 = total_cer_2 / num_batches\n",
        "        avg_wer_2 = total_wer_2 / num_batches\n",
        "\n",
        "        return {\n",
        "            'loss_1': avg_loss_1,\n",
        "            'loss_2': avg_loss_2,\n",
        "            'accuracy_1': avg_accuracy_1,\n",
        "            'accuracy_2': avg_accuracy_2,\n",
        "            'perplexity_1': avg_perplexity_1,\n",
        "            'perplexity_2': avg_perplexity_2,\n",
        "            'bleu_1': avg_bleu_1,\n",
        "            'bleu_2': avg_bleu_2,\n",
        "            'cer_1': avg_cer_1,\n",
        "            'cer_2': avg_cer_2,\n",
        "            'wer_1': avg_wer_1,\n",
        "            'wer_2': avg_wer_2\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, train_loader_1: DataLoader, train_loader_2: DataLoader, val_loader_1: DataLoader = None,\n",
        "                   val_loader_2: DataLoader = None, print_every: int = 100) -> Dict[str, float]:\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "\n",
        "        epoch_loss_1 = 0\n",
        "        epoch_loss_2 = 0\n",
        "        epoch_accuracy_1 = 0\n",
        "        epoch_accuracy_2 = 0\n",
        "        epoch_tokens_1 = 0\n",
        "        epoch_tokens_2 = 0\n",
        "\n",
        "        progress_bar = tqdm(zip(train_loader_1, train_loader_2), desc=\"Training\")\n",
        "\n",
        "        for batch_idx, (batch_1, batch_2) in enumerate(progress_bar):\n",
        "            # Training step\n",
        "            metrics_1 = self.train_step(batch_1, flag = True)\n",
        "            metrics_2 = self.train_step(batch_2, flag = False)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            epoch_loss_1 += metrics_1['loss']\n",
        "            epoch_loss_2 += metrics_2['loss']\n",
        "            epoch_accuracy_1 += metrics_1['accuracy'] * metrics_1['num_tokens']\n",
        "            epoch_accuracy_2 += metrics_2['accuracy'] * metrics_2['num_tokens']\n",
        "            epoch_tokens_1 += metrics_1['num_tokens']\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss_1': f\"{metrics_1['loss']:.4f}\",\n",
        "                'loss_2': f\"{metrics_2['loss']:.4f}\",\n",
        "                'acc_1': f\"{metrics_1['accuracy']:.4f}\",\n",
        "                'acc_2': f\"{metrics_2['accuracy']:.4f}\",\n",
        "                'lr': f\"{metrics_1['lr']:.2e}\",\n",
        "                'ppl_1': f\"{metrics_1['perplexity']:.2f}\",\n",
        "                'ppl_2': f\"{metrics_2['perplexity']:.2f}\"\n",
        "            })\n",
        "\n",
        "            # Print detailed stats\n",
        "            if (batch_idx + 1) % print_every == 0:\n",
        "                avg_loss_1 = epoch_loss_1 / (batch_idx + 1)\n",
        "                avg_loss_2 = epoch_loss_2 / (batch_idx + 1)\n",
        "                avg_acc_1 = epoch_accuracy_1 / epoch_tokens_1 if epoch_tokens_1 > 0 else 0\n",
        "                avg_acc_2 = epoch_accuracy_2 / epoch_tokens_2 if epoch_tokens_2 > 0 else 0\n",
        "\n",
        "                print(f\"\\nStep {self.step}, Batch {batch_idx + 1}/{len(train_loader_1)}\")\n",
        "                print(f\"Loss 1: {avg_loss_1:.4f}, Accuracy: {avg_acc_1:.4f}\")\n",
        "                print(f\"Loss 2: {avg_loss_2:.4f}, Accuracy: {avg_acc_2:.4f}\")\n",
        "                print(f\"Learning Rate: {metrics_1['lr']:.2e}\")\n",
        "\n",
        "        # Calculate epoch averages\n",
        "        avg_train_loss_1 = epoch_loss_1 / len(train_loader_1)\n",
        "        avg_train_loss_2 = epoch_loss_2 / len(train_loader_2)\n",
        "        avg_train_acc_1 = epoch_accuracy_1 / epoch_tokens_1 if epoch_tokens_1 > 0 else 0\n",
        "        avg_train_acc_2 = epoch_accuracy_2 / epoch_tokens_2 if epoch_tokens_2 > 0 else 0\n",
        "\n",
        "        # Validation\n",
        "        val_metrics = {}\n",
        "        if (val_loader_1 is not None) and (val_loader_2 is not None):\n",
        "            print(\"\\nRunning validation...\")\n",
        "            val_metrics = self.validate(val_loader_1, val_loader_2)\n",
        "\n",
        "        # Store metrics\n",
        "        self.train_losses_1.append(avg_train_loss_1)\n",
        "        self.train_losses_2.append(avg_train_loss_2)\n",
        "        self.train_accuracies_1.append(avg_train_acc_1)\n",
        "        self.train_accuracies_2.append(avg_train_acc_2)\n",
        "        if val_metrics:\n",
        "            self.val_losses_1.append(val_metrics['loss_1'])\n",
        "            self.val_losses_2.append(val_metrics['loss_2'])\n",
        "            self.val_accuracies_1.append(val_metrics['accuracy_1'])\n",
        "            self.val_accuracies_2.append(val_metrics['accuracy_2'])\n",
        "            self.val_bleu_1.append(val_metrics['bleu_1'])\n",
        "            self.val_bleu_2.append(val_metrics['bleu_2'])\n",
        "            self.val_cer_1.append(val_metrics['cer_1'])\n",
        "            self.val_cer_2.append(val_metrics['cer_2'])\n",
        "            self.val_wer_1.append(val_metrics['wer_1'])\n",
        "            self.val_wer_2.append(val_metrics['wer_2'])\n",
        "\n",
        "        return {\n",
        "            'train_loss_1': avg_train_loss_1,\n",
        "            'train_loss_2': avg_train_loss_2,\n",
        "            'train_accuracy_1': avg_train_acc_1,\n",
        "            'train_accuracy_2': avg_train_acc_2,\n",
        "            **val_metrics\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, filepath: str, epoch: int, best_val_loss_1: float = None, best_val_loss_2: float = None):\n",
        "        \"\"\"Save model checkpoint.\"\"\"\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'train_losses_1': self.train_losses_1,\n",
        "            'train_losses_2': self.train_losses_2,\n",
        "            'train_accuracies_1': self.train_accuracies_1,\n",
        "            'train_accuracies_2': self.train_accuracies_2,\n",
        "            'val_losses_1': self.val_losses_1,\n",
        "            'val_losses_2': self.val_losses_2,\n",
        "            'val_accuracies_1': self.val_accuracies_1,\n",
        "            'val_accuracies_2': self.val_accuracies_2,\n",
        "            'val_bleu_1': self.val_bleu_1,\n",
        "            'val_bleu_2': self.val_bleu_2,\n",
        "            'val_cer_1': self.val_cer_1,\n",
        "            'val_cer_2': self.val_cer_2,\n",
        "            'val_wer_1': self.val_wer_1,\n",
        "            'val_wer_2': self.val_wer_2,\n",
        "            'step': self.step,\n",
        "            'best_val_loss_1': best_val_loss_1,\n",
        "            'best_val_loss_2': best_val_loss_2\n",
        "        }\n",
        "        torch.save(checkpoint, filepath)\n",
        "\n",
        "    def load_checkpoint(self, filepath: str):\n",
        "        \"\"\"Load model checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(filepath, map_location=self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.train_losses_1 = checkpoint.get('train_losses_1', [])\n",
        "        self.train_losses_2 = checkpoint.get('train_losses_2', [])\n",
        "        self.train_accuracies_1 = checkpoint.get('train_accuracies_1', [])\n",
        "        self.train_accuracies_2 = checkpoint.get('train_accuracies_2', [])\n",
        "        self.val_losses_1 = checkpoint.get('val_losses_1', [])\n",
        "        self.val_losses_2 = checkpoint.get('val_losses_2', [])\n",
        "        self.val_accuracies_1 = checkpoint.get('val_accuracies_1', [])\n",
        "        self.val_accuracies_2 = checkpoint.get('val_accuracies_2', [])\n",
        "        self.val_bleu_1 = checkpoint.get('val_bleu_1', [])\n",
        "        self.val_bleu_2 = checkpoint.get('val_bleu_2', [])\n",
        "        self.val_cer_1 = checkpoint.get('val_cer_1', [])\n",
        "        self.val_cer_2 = checkpoint.get('val_cer_2', [])\n",
        "        self.val_wer_1 = checkpoint.get('val_wer_1', [])\n",
        "        self.val_wer_2 = checkpoint.get('val_wer_2', [])\n",
        "        self.step = checkpoint.get('step', 0)\n",
        "        return checkpoint.get('epoch', 0), checkpoint.get('best_val_loss_1', float('inf')), checkpoint.get('best_val_loss_2', float('inf'))\n",
        "\n",
        "    def plot_metrics(self, save_path: str = None):\n",
        "        \"\"\"Plot training metrics.\"\"\"\n",
        "        fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "        epochs = range(1, len(self.train_losses_1) + 1)\n",
        "\n",
        "        # Loss plot\n",
        "        ax1.plot(epochs, self.train_losses_1, 'b-', label='Train Loss 1')\n",
        "        ax1.plot(epochs, self.train_losses_2, 'c-', label='Train Loss 2')\n",
        "\n",
        "        if self.val_losses_1:\n",
        "            ax1.plot(epochs, self.val_losses_1, 'r-', label='Val Loss 1')\n",
        "            ax1.plot(epochs, self.val_losses_2, 'g-', label='Val Loss 2')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.set_title('Training and Validation Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Accuracy plot\n",
        "        ax2.plot(epochs, self.train_accuracies_1, 'b-', label='Train Accuracy 1')\n",
        "        ax2.plot(epochs, self.train_accuracies_2, 'c-', label='Train Accuracy 2')\n",
        "        if self.val_accuracies_1:\n",
        "            ax2.plot(epochs, self.val_accuracies_1, 'r-', label='Val Accuracy 1')\n",
        "            ax2.plot(epochs, self.val_accuracies_2, 'g-', label='Val Accuracy 2')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.set_title('Training and Validation Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "\n",
        "def train_transformer(model, train_loader_1, train_loader_2, val_loader_1, val_loader_2, dataset,\n",
        "                     num_epochs=10, save_dir='./checkpoints'):\n",
        "    \"\"\"\n",
        "    Main training function.\n",
        "    \"\"\"\n",
        "    # Create save directory\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Move model to device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = TransformerTrainer(\n",
        "        model=model,\n",
        "        vocab_size=len(dataset.vocab),\n",
        "        idx_to_token=dataset.idx_to_token,\n",
        "        pad_idx=dataset.token_to_idx[dataset.PAD_TOKEN],\n",
        "        learning_rate=1e-4,\n",
        "        label_smoothing=0.1\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss_1, best_val_loss_2 = float('inf'), float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Train epoch\n",
        "        start_time = time.time()\n",
        "        epoch_metrics = trainer.train_epoch(train_loader_1, train_loader_2, val_loader_1, val_loader_2)\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        print(f\"\\nEpoch {epoch} Summary:\")\n",
        "        print(f\"Time: {epoch_time:.1f}s\")\n",
        "        print(\"Training Metrics:\")\n",
        "        print(f\"  Loss 1: {epoch_metrics['train_loss_1']:.4f}, Loss 2: {epoch_metrics['train_loss_2']:.4f}\")\n",
        "        print(f\"  Accuracy 1: {epoch_metrics['train_accuracy_1']:.4f}, Accuracy 2: {epoch_metrics['train_accuracy_2']:.4f}\")\n",
        "\n",
        "        if 'loss_1' in epoch_metrics:  # Validation metrics available\n",
        "            val_loss_1 = epoch_metrics['loss_1']\n",
        "            val_loss_2 = epoch_metrics['loss_2']\n",
        "            print(\"Validation Metrics:\")\n",
        "            print(f\"  Loss 1: {val_loss_1:.4f}, Loss 2: {val_loss_2:.4f}\")\n",
        "            print(f\"  Accuracy 1: {epoch_metrics['accuracy_1']:.4f}, Accuracy 2: {epoch_metrics['accuracy_2']:.4f}\")\n",
        "            print(f\"  BLEU 1: {epoch_metrics['bleu_1']:.4f}, BLEU 2: {epoch_metrics['bleu_2']:.4f}\")\n",
        "            print(f\"  CER 1: {epoch_metrics['cer_1']:.4f}, CER 2: {epoch_metrics['cer_2']:.4f}\")\n",
        "            print(f\"  WER 1: {epoch_metrics['wer_1']:.4f}, WER 2: {epoch_metrics['wer_2']:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss_1 < best_val_loss_1 and val_loss_2 < best_val_loss_2:\n",
        "                best_val_loss_1 = val_loss_1\n",
        "                best_val_loss_2 = val_loss_2\n",
        "                patience_counter = 0\n",
        "                best_model_path = os.path.join(save_dir, 'best_model.pt')\n",
        "                trainer.save_checkpoint(best_model_path, epoch, best_val_loss_1, best_val_loss_2)\n",
        "                print(f\"New best model saved!\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping! No improvement for {patience} epochs.\")\n",
        "                    break\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}.pt')\n",
        "        trainer.save_checkpoint(checkpoint_path, epoch, best_val_loss_1, best_val_loss_2)\n",
        "        trainer.plot_metrics(os.path.join(save_dir, 'training_curves.png'))\n",
        "\n",
        "    # Plot training curves\n",
        "    print(\"\\nTraining completed!\")\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "SLseZaYMTysO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "7kcx0B9fjIiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load both training datasets (C + C++) without vocab\n",
        "temp_dataset_c = TokenizedCodeDataset(\"c_code_train.txt\", vocab_size=10000)\n",
        "temp_dataset_cpp = TokenizedCodeDataset(\"cpp_code_train.txt\", vocab_size=10000)\n",
        "\n",
        "# Step 2: Merge vocab counts and build shared vocab\n",
        "all_tokens = Counter()\n",
        "for seq in temp_dataset_c.sequences + temp_dataset_cpp.sequences:\n",
        "    all_tokens.update(seq)\n",
        "\n",
        "# Build shared vocab manually\n",
        "special_tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
        "most_common = [tok for tok, _ in all_tokens.most_common(10000 - len(special_tokens))]\n",
        "shared_vocab = special_tokens + most_common\n",
        "\n",
        "dataloader_1, dataset1 = create_dataloader(\n",
        "    file_path=\"c_code_train.txt\",\n",
        "    batch_size=8,\n",
        "    max_seq_len=1000,\n",
        "    vocab_size=None,   # ignored since we pass vocab\n",
        "    vocab=shared_vocab\n",
        ")\n",
        "\n",
        "dataloader_2, dataset2 = create_dataloader(\n",
        "    file_path=\"cpp_code_train.txt\",\n",
        "    batch_size=8,\n",
        "    max_seq_len=1000,\n",
        "    vocab=shared_vocab\n",
        ")\n",
        "\n",
        "valloader_1, dataset3 = create_dataloader(\n",
        "    file_path=\"c_code_val.txt\",\n",
        "    batch_size=8,\n",
        "    max_seq_len=1000,\n",
        "    vocab=shared_vocab\n",
        ")\n",
        "\n",
        "valloader_2, dataset4 = create_dataloader(\n",
        "    file_path=\"cpp_code_val.txt\",\n",
        "    batch_size=8,\n",
        "    max_seq_len=1000,\n",
        "    vocab=shared_vocab\n",
        ")\n",
        "\n",
        "\n",
        "model = Model1(src_vocab_size=len(dataset1.vocab),\n",
        "        tgt_vocab_size=len(dataset1.vocab),\n",
        "        d_model=256,\n",
        "        nhead=4,\n",
        "        num_encoder_layers=3,\n",
        "        num_decoder_layers=3)\n",
        "\n",
        "trainer = train_transformer(model, dataloader_1, dataloader_2, valloader_1, valloader_2, dataset1, 2)\n"
      ],
      "metadata": {
        "id": "peaZL8BwYuPr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4700ca4d-ae5b-4427-aeb0-67d5e171f19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 96000 sequences\n",
            "Vocabulary size: 10000\n",
            "Average sequence length: 383.7\n",
            "Loaded 49853 sequences\n",
            "Vocabulary size: 10000\n",
            "Average sequence length: 337.9\n",
            "Loaded 96000 sequences\n",
            "Vocabulary size: 10000\n",
            "Average sequence length: 426.3\n",
            "Loaded 49853 sequences\n",
            "Vocabulary size: 10000\n",
            "Average sequence length: 403.4\n",
            "Loaded 12000 sequences\n",
            "Vocabulary size: 10000\n",
            "Average sequence length: 425.3\n",
            "Loaded 6232 sequences\n",
            "Vocabulary size: 10000\n",
            "Average sequence length: 411.7\n",
            "Training on device: cuda\n",
            "Starting training for 2 epochs...\n",
            "Model parameters: 26,237,728\n",
            "\n",
            "============================================================\n",
            "Epoch 1/2\n",
            "============================================================\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 0it [00:00, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6041: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "Training: 100it [00:51,  1.97it/s, loss_1=9.3312, loss_2=9.3304, acc_1=0.0000, acc_2=0.0003, lr=4.92e-09, ppl_1=11284.36, ppl_2=11276.11]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 200, Batch 100/12000\n",
            "Loss 1: 9.3462, Accuracy: 0.0000\n",
            "Loss 2: 9.3069, Accuracy: 0.0000\n",
            "Learning Rate: 4.92e-09\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 200it [01:42,  2.17it/s, loss_1=9.3759, loss_2=9.2824, acc_1=0.0003, acc_2=0.0003, lr=9.86e-09, ppl_1=11800.62, ppl_2=10747.26]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 400, Batch 200/12000\n",
            "Loss 1: 9.3451, Accuracy: 0.0000\n",
            "Loss 2: 9.3068, Accuracy: 0.0000\n",
            "Learning Rate: 9.86e-09\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 300it [02:33,  1.67it/s, loss_1=9.3474, loss_2=9.2971, acc_1=0.0000, acc_2=0.0003, lr=1.48e-08, ppl_1=11469.49, ppl_2=10906.37]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 600, Batch 300/12000\n",
            "Loss 1: 9.3438, Accuracy: 0.0000\n",
            "Loss 2: 9.3051, Accuracy: 0.0000\n",
            "Learning Rate: 1.48e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 400it [03:28,  2.08it/s, loss_1=9.3725, loss_2=9.3025, acc_1=0.0000, acc_2=0.0003, lr=1.97e-08, ppl_1=11760.96, ppl_2=10965.05]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 800, Batch 400/12000\n",
            "Loss 1: 9.3407, Accuracy: 0.0000\n",
            "Loss 2: 9.3027, Accuracy: 0.0000\n",
            "Learning Rate: 1.97e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 500it [04:21,  2.10it/s, loss_1=9.3194, loss_2=9.2647, acc_1=0.0000, acc_2=0.0000, lr=2.47e-08, ppl_1=11152.65, ppl_2=10558.94]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1000, Batch 500/12000\n",
            "Loss 1: 9.3380, Accuracy: 0.0000\n",
            "Loss 2: 9.2996, Accuracy: 0.0000\n",
            "Learning Rate: 2.47e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 600it [05:14,  1.69it/s, loss_1=9.3202, loss_2=9.2623, acc_1=0.0003, acc_2=0.0000, lr=2.96e-08, ppl_1=11160.80, ppl_2=10533.10]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1200, Batch 600/12000\n",
            "Loss 1: 9.3340, Accuracy: 0.0000\n",
            "Loss 2: 9.2959, Accuracy: 0.0000\n",
            "Learning Rate: 2.96e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 700it [06:07,  1.69it/s, loss_1=9.2919, loss_2=9.2550, acc_1=0.0000, acc_2=0.0000, lr=3.46e-08, ppl_1=10849.44, ppl_2=10456.43]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1400, Batch 700/12000\n",
            "Loss 1: 9.3301, Accuracy: 0.0000\n",
            "Loss 2: 9.2907, Accuracy: 0.0000\n",
            "Learning Rate: 3.46e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 800it [06:59,  1.81it/s, loss_1=9.2887, loss_2=9.2313, acc_1=0.0004, acc_2=0.0000, lr=3.95e-08, ppl_1=10815.25, ppl_2=10212.05]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1600, Batch 800/12000\n",
            "Loss 1: 9.3246, Accuracy: 0.0000\n",
            "Loss 2: 9.2851, Accuracy: 0.0000\n",
            "Learning Rate: 3.95e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 900it [07:54,  1.67it/s, loss_1=9.2578, loss_2=9.2251, acc_1=0.0000, acc_2=0.0000, lr=4.44e-08, ppl_1=10486.36, ppl_2=10149.07]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 1800, Batch 900/12000\n",
            "Loss 1: 9.3184, Accuracy: 0.0000\n",
            "Loss 2: 9.2790, Accuracy: 0.0000\n",
            "Learning Rate: 4.44e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1000it [08:46,  1.97it/s, loss_1=9.2842, loss_2=9.2326, acc_1=0.0000, acc_2=0.0000, lr=4.94e-08, ppl_1=10766.96, ppl_2=10224.64]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 2000, Batch 1000/12000\n",
            "Loss 1: 9.3110, Accuracy: 0.0000\n",
            "Loss 2: 9.2723, Accuracy: 0.0000\n",
            "Learning Rate: 4.94e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1100it [09:40,  1.91it/s, loss_1=9.2125, loss_2=9.1500, acc_1=0.0000, acc_2=0.0000, lr=5.43e-08, ppl_1=10021.27, ppl_2=9414.48]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 2200, Batch 1100/12000\n",
            "Loss 1: 9.3031, Accuracy: 0.0000\n",
            "Loss 2: 9.2645, Accuracy: 0.0000\n",
            "Learning Rate: 5.43e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1200it [10:34,  1.58it/s, loss_1=9.1858, loss_2=9.1484, acc_1=0.0000, acc_2=0.0002, lr=5.93e-08, ppl_1=9757.56, ppl_2=9399.29]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 2400, Batch 1200/12000\n",
            "Loss 1: 9.2944, Accuracy: 0.0000\n",
            "Loss 2: 9.2560, Accuracy: 0.0000\n",
            "Learning Rate: 5.93e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1300it [11:25,  1.83it/s, loss_1=9.1819, loss_2=9.1308, acc_1=0.0002, acc_2=0.0000, lr=6.42e-08, ppl_1=9719.45, ppl_2=9235.75]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 2600, Batch 1300/12000\n",
            "Loss 1: 9.2851, Accuracy: 0.0000\n",
            "Loss 2: 9.2469, Accuracy: 0.0000\n",
            "Learning Rate: 6.42e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1400it [12:16,  1.85it/s, loss_1=9.1323, loss_2=9.1092, acc_1=0.0000, acc_2=0.0007, lr=6.92e-08, ppl_1=9249.11, ppl_2=9037.64]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 2800, Batch 1400/12000\n",
            "Loss 1: 9.2747, Accuracy: 0.0001\n",
            "Loss 2: 9.2372, Accuracy: 0.0000\n",
            "Learning Rate: 6.92e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1500it [13:09,  1.79it/s, loss_1=9.0716, loss_2=9.0471, acc_1=0.0008, acc_2=0.0007, lr=7.41e-08, ppl_1=8704.12, ppl_2=8494.11]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3000, Batch 1500/12000\n",
            "Loss 1: 9.2638, Accuracy: 0.0001\n",
            "Loss 2: 9.2267, Accuracy: 0.0000\n",
            "Learning Rate: 7.41e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1600it [14:02,  2.02it/s, loss_1=9.0581, loss_2=9.0596, acc_1=0.0008, acc_2=0.0003, lr=7.90e-08, ppl_1=8587.67, ppl_2=8600.67]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3200, Batch 1600/12000\n",
            "Loss 1: 9.2522, Accuracy: 0.0001\n",
            "Loss 2: 9.2156, Accuracy: 0.0000\n",
            "Learning Rate: 7.90e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1700it [14:54,  2.08it/s, loss_1=8.9938, loss_2=9.0410, acc_1=0.0019, acc_2=0.0000, lr=8.40e-08, ppl_1=8052.84, ppl_2=8442.52]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3400, Batch 1700/12000\n",
            "Loss 1: 9.2401, Accuracy: 0.0002\n",
            "Loss 2: 9.2036, Accuracy: 0.0000\n",
            "Learning Rate: 8.40e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1800it [15:47,  1.78it/s, loss_1=8.9887, loss_2=8.9669, acc_1=0.0053, acc_2=0.0002, lr=8.89e-08, ppl_1=8012.29, ppl_2=7839.35]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3600, Batch 1800/12000\n",
            "Loss 1: 9.2273, Accuracy: 0.0004\n",
            "Loss 2: 9.1914, Accuracy: 0.0000\n",
            "Learning Rate: 8.89e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 1900it [16:40,  2.03it/s, loss_1=8.9963, loss_2=8.9412, acc_1=0.0092, acc_2=0.0035, lr=9.39e-08, ppl_1=8073.54, ppl_2=7640.54]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 3800, Batch 1900/12000\n",
            "Loss 1: 9.2141, Accuracy: 0.0006\n",
            "Loss 2: 9.1785, Accuracy: 0.0000\n",
            "Learning Rate: 9.39e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2000it [17:33,  1.94it/s, loss_1=8.9202, loss_2=8.8818, acc_1=0.0106, acc_2=0.0062, lr=9.88e-08, ppl_1=7481.87, ppl_2=7200.09]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4000, Batch 2000/12000\n",
            "Loss 1: 9.2003, Accuracy: 0.0011\n",
            "Loss 2: 9.1649, Accuracy: 0.0000\n",
            "Learning Rate: 9.88e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2100it [18:25,  1.97it/s, loss_1=8.8925, loss_2=8.8255, acc_1=0.0117, acc_2=0.0111, lr=9.65e-08, ppl_1=7277.12, ppl_2=6805.87]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4200, Batch 2100/12000\n",
            "Loss 1: 9.1857, Accuracy: 0.0017\n",
            "Loss 2: 9.1513, Accuracy: 0.0000\n",
            "Learning Rate: 9.65e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2200it [19:18,  2.03it/s, loss_1=8.8388, loss_2=8.8232, acc_1=0.0168, acc_2=0.0135, lr=9.42e-08, ppl_1=6896.93, ppl_2=6789.76]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4400, Batch 2200/12000\n",
            "Loss 1: 9.1709, Accuracy: 0.0025\n",
            "Loss 2: 9.1368, Accuracy: 0.0000\n",
            "Learning Rate: 9.42e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2300it [20:11,  2.21it/s, loss_1=8.7998, loss_2=8.7607, acc_1=0.0265, acc_2=0.0293, lr=9.22e-08, ppl_1=6633.06, ppl_2=6378.73]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4600, Batch 2300/12000\n",
            "Loss 1: 9.1559, Accuracy: 0.0035\n",
            "Loss 2: 9.1221, Accuracy: 0.0000\n",
            "Learning Rate: 9.22e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2400it [21:03,  1.72it/s, loss_1=8.8417, loss_2=8.7263, acc_1=0.0267, acc_2=0.0278, lr=9.02e-08, ppl_1=6916.45, ppl_2=6163.04]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 4800, Batch 2400/12000\n",
            "Loss 1: 9.1409, Accuracy: 0.0047\n",
            "Loss 2: 9.1072, Accuracy: 0.0000\n",
            "Learning Rate: 9.02e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2500it [21:57,  1.72it/s, loss_1=8.6804, loss_2=8.6909, acc_1=0.0521, acc_2=0.0283, lr=8.84e-08, ppl_1=5886.32, ppl_2=5948.28]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 5000, Batch 2500/12000\n",
            "Loss 1: 9.1258, Accuracy: 0.0060\n",
            "Loss 2: 9.0925, Accuracy: 0.0000\n",
            "Learning Rate: 8.84e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2600it [22:50,  1.61it/s, loss_1=8.7387, loss_2=8.6931, acc_1=0.0557, acc_2=0.0421, lr=8.67e-08, ppl_1=6239.62, ppl_2=5961.34]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 5200, Batch 2600/12000\n",
            "Loss 1: 9.1108, Accuracy: 0.0076\n",
            "Loss 2: 9.0779, Accuracy: 0.0000\n",
            "Learning Rate: 8.67e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2700it [23:42,  2.03it/s, loss_1=8.7138, loss_2=8.6533, acc_1=0.0447, acc_2=0.0431, lr=8.51e-08, ppl_1=6086.18, ppl_2=5728.80]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 5400, Batch 2700/12000\n",
            "Loss 1: 9.0957, Accuracy: 0.0095\n",
            "Loss 2: 9.0630, Accuracy: 0.0000\n",
            "Learning Rate: 8.51e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2800it [24:35,  2.20it/s, loss_1=8.6307, loss_2=8.6377, acc_1=0.0745, acc_2=0.0446, lr=8.35e-08, ppl_1=5601.21, ppl_2=5640.39]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 5600, Batch 2800/12000\n",
            "Loss 1: 9.0809, Accuracy: 0.0115\n",
            "Loss 2: 9.0482, Accuracy: 0.0000\n",
            "Learning Rate: 8.35e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 2900it [25:29,  1.89it/s, loss_1=8.6330, loss_2=8.6365, acc_1=0.0692, acc_2=0.0447, lr=8.21e-08, ppl_1=5613.90, ppl_2=5633.83]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 5800, Batch 2900/12000\n",
            "Loss 1: 9.0662, Accuracy: 0.0136\n",
            "Loss 2: 9.0337, Accuracy: 0.0000\n",
            "Learning Rate: 8.21e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 3000it [26:23,  2.03it/s, loss_1=8.6509, loss_2=8.6130, acc_1=0.0793, acc_2=0.0616, lr=8.07e-08, ppl_1=5715.13, ppl_2=5502.53]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 6000, Batch 3000/12000\n",
            "Loss 1: 9.0517, Accuracy: 0.0158\n",
            "Loss 2: 9.0191, Accuracy: 0.0000\n",
            "Learning Rate: 8.07e-08\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 3100it [27:18,  1.73it/s, loss_1=8.5619, loss_2=8.5206, acc_1=0.0952, acc_2=0.0714, lr=7.94e-08, ppl_1=5228.65, ppl_2=5017.23]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Step 6200, Batch 3100/12000\n",
            "Loss 1: 9.0376, Accuracy: 0.0180\n",
            "Loss 2: 9.0049, Accuracy: 0.0000\n",
            "Learning Rate: 7.94e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3200it [28:09,  2.16it/s, loss_1=8.5201, loss_2=8.4854, acc_1=0.0829, acc_2=0.0818, lr=7.81e-08, ppl_1=5014.44, ppl_2=4843.56]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6400, Batch 3200/12000\n",
            "Loss 1: 9.0235, Accuracy: 0.0201\n",
            "Loss 2: 8.9908, Accuracy: 0.0000\n",
            "Learning Rate: 7.81e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3300it [29:02,  2.10it/s, loss_1=8.5831, loss_2=8.3878, acc_1=0.0953, acc_2=0.1013, lr=7.69e-08, ppl_1=5340.70, ppl_2=4393.05]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6600, Batch 3300/12000\n",
            "Loss 1: 9.0098, Accuracy: 0.0222\n",
            "Loss 2: 8.9769, Accuracy: 0.0000\n",
            "Learning Rate: 7.69e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3400it [29:55,  2.20it/s, loss_1=8.5743, loss_2=8.4134, acc_1=0.0900, acc_2=0.0955, lr=7.58e-08, ppl_1=5293.93, ppl_2=4507.01]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6800, Batch 3400/12000\n",
            "Loss 1: 8.9962, Accuracy: 0.0242\n",
            "Loss 2: 8.9631, Accuracy: 0.0000\n",
            "Learning Rate: 7.58e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3500it [30:47,  1.63it/s, loss_1=8.6086, loss_2=8.5601, acc_1=0.0862, acc_2=0.0668, lr=7.47e-08, ppl_1=5478.58, ppl_2=5219.15]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7000, Batch 3500/12000\n",
            "Loss 1: 8.9827, Accuracy: 0.0261\n",
            "Loss 2: 8.9495, Accuracy: 0.0000\n",
            "Learning Rate: 7.47e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3600it [31:43,  1.89it/s, loss_1=8.5420, loss_2=8.4353, acc_1=0.0918, acc_2=0.0854, lr=7.37e-08, ppl_1=5125.64, ppl_2=4607.02]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7200, Batch 3600/12000\n",
            "Loss 1: 8.9696, Accuracy: 0.0279\n",
            "Loss 2: 8.9362, Accuracy: 0.0000\n",
            "Learning Rate: 7.37e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3700it [32:37,  2.13it/s, loss_1=8.5221, loss_2=8.4384, acc_1=0.0895, acc_2=0.0855, lr=7.27e-08, ppl_1=5024.65, ppl_2=4621.02]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7400, Batch 3700/12000\n",
            "Loss 1: 8.9567, Accuracy: 0.0297\n",
            "Loss 2: 8.9232, Accuracy: 0.0000\n",
            "Learning Rate: 7.27e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3800it [33:30,  1.73it/s, loss_1=8.4085, loss_2=8.4906, acc_1=0.1017, acc_2=0.0769, lr=7.17e-08, ppl_1=4484.85, ppl_2=4868.86]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7600, Batch 3800/12000\n",
            "Loss 1: 8.9439, Accuracy: 0.0314\n",
            "Loss 2: 8.9102, Accuracy: 0.0000\n",
            "Learning Rate: 7.17e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3900it [34:22,  1.90it/s, loss_1=8.4931, loss_2=8.3712, acc_1=0.0950, acc_2=0.0978, lr=7.08e-08, ppl_1=4881.22, ppl_2=4320.79]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7800, Batch 3900/12000\n",
            "Loss 1: 8.9313, Accuracy: 0.0330\n",
            "Loss 2: 8.8975, Accuracy: 0.0000\n",
            "Learning Rate: 7.08e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4000it [35:15,  1.97it/s, loss_1=8.4324, loss_2=8.3795, acc_1=0.0957, acc_2=0.0883, lr=6.99e-08, ppl_1=4593.74, ppl_2=4356.82]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8000, Batch 4000/12000\n",
            "Loss 1: 8.9191, Accuracy: 0.0345\n",
            "Loss 2: 8.8851, Accuracy: 0.0000\n",
            "Learning Rate: 6.99e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4100it [36:08,  1.61it/s, loss_1=8.4404, loss_2=8.4675, acc_1=0.0873, acc_2=0.0808, lr=6.90e-08, ppl_1=4630.30, ppl_2=4757.70]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8200, Batch 4100/12000\n",
            "Loss 1: 8.9071, Accuracy: 0.0359\n",
            "Loss 2: 8.8729, Accuracy: 0.0000\n",
            "Learning Rate: 6.90e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4200it [37:02,  1.79it/s, loss_1=8.3880, loss_2=8.3546, acc_1=0.0903, acc_2=0.0812, lr=6.82e-08, ppl_1=4393.82, ppl_2=4249.60]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8400, Batch 4200/12000\n",
            "Loss 1: 8.8951, Accuracy: 0.0374\n",
            "Loss 2: 8.8610, Accuracy: 0.0000\n",
            "Learning Rate: 6.82e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4300it [37:56,  1.75it/s, loss_1=8.4154, loss_2=8.4489, acc_1=0.0900, acc_2=0.0812, lr=6.74e-08, ppl_1=4516.01, ppl_2=4669.90]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8600, Batch 4300/12000\n",
            "Loss 1: 8.8835, Accuracy: 0.0387\n",
            "Loss 2: 8.8495, Accuracy: 0.0000\n",
            "Learning Rate: 6.74e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4400it [38:50,  2.10it/s, loss_1=8.3611, loss_2=8.2987, acc_1=0.0987, acc_2=0.0901, lr=6.66e-08, ppl_1=4277.52, ppl_2=4018.60]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8800, Batch 4400/12000\n",
            "Loss 1: 8.8722, Accuracy: 0.0399\n",
            "Loss 2: 8.8382, Accuracy: 0.0000\n",
            "Learning Rate: 6.66e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4500it [39:44,  2.14it/s, loss_1=8.3819, loss_2=8.1980, acc_1=0.1002, acc_2=0.1130, lr=6.59e-08, ppl_1=4367.42, ppl_2=3633.67]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9000, Batch 4500/12000\n",
            "Loss 1: 8.8610, Accuracy: 0.0411\n",
            "Loss 2: 8.8267, Accuracy: 0.0000\n",
            "Learning Rate: 6.59e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4600it [40:37,  1.95it/s, loss_1=8.4416, loss_2=8.2464, acc_1=0.0797, acc_2=0.1024, lr=6.52e-08, ppl_1=4636.16, ppl_2=3813.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9200, Batch 4600/12000\n",
            "Loss 1: 8.8501, Accuracy: 0.0422\n",
            "Loss 2: 8.8155, Accuracy: 0.0000\n",
            "Learning Rate: 6.52e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4700it [41:29,  1.58it/s, loss_1=8.3381, loss_2=8.2428, acc_1=0.1034, acc_2=0.0996, lr=6.45e-08, ppl_1=4180.10, ppl_2=3800.13]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9400, Batch 4700/12000\n",
            "Loss 1: 8.8391, Accuracy: 0.0433\n",
            "Loss 2: 8.8045, Accuracy: 0.0000\n",
            "Learning Rate: 6.45e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4800it [42:23,  1.65it/s, loss_1=8.4088, loss_2=8.2785, acc_1=0.0838, acc_2=0.0904, lr=6.38e-08, ppl_1=4486.58, ppl_2=3938.25]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9600, Batch 4800/12000\n",
            "Loss 1: 8.8285, Accuracy: 0.0443\n",
            "Loss 2: 8.7936, Accuracy: 0.0000\n",
            "Learning Rate: 6.38e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4900it [43:15,  1.94it/s, loss_1=8.3156, loss_2=8.3130, acc_1=0.0919, acc_2=0.0943, lr=6.31e-08, ppl_1=4087.11, ppl_2=4076.51]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9800, Batch 4900/12000\n",
            "Loss 1: 8.8182, Accuracy: 0.0453\n",
            "Loss 2: 8.7829, Accuracy: 0.0000\n",
            "Learning Rate: 6.31e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5000it [44:09,  1.67it/s, loss_1=8.3474, loss_2=8.2507, acc_1=0.0875, acc_2=0.0974, lr=6.25e-08, ppl_1=4219.38, ppl_2=3830.28]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 10000, Batch 5000/12000\n",
            "Loss 1: 8.8078, Accuracy: 0.0463\n",
            "Loss 2: 8.7726, Accuracy: 0.0000\n",
            "Learning Rate: 6.25e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5100it [45:03,  1.93it/s, loss_1=8.2195, loss_2=8.2583, acc_1=0.1046, acc_2=0.0989, lr=6.19e-08, ppl_1=3712.76, ppl_2=3859.45]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 10200, Batch 5100/12000\n",
            "Loss 1: 8.7978, Accuracy: 0.0472\n",
            "Loss 2: 8.7626, Accuracy: 0.0000\n",
            "Learning Rate: 6.19e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5200it [45:56,  1.56it/s, loss_1=8.2394, loss_2=8.2519, acc_1=0.0954, acc_2=0.0986, lr=6.13e-08, ppl_1=3787.19, ppl_2=3835.07]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 10400, Batch 5200/12000\n",
            "Loss 1: 8.7879, Accuracy: 0.0481\n",
            "Loss 2: 8.7526, Accuracy: 0.0000\n",
            "Learning Rate: 6.13e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5300it [46:48,  1.80it/s, loss_1=8.2744, loss_2=8.2850, acc_1=0.0939, acc_2=0.1005, lr=6.07e-08, ppl_1=3922.36, ppl_2=3963.80]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 10600, Batch 5300/12000\n",
            "Loss 1: 8.7780, Accuracy: 0.0490\n",
            "Loss 2: 8.7426, Accuracy: 0.0000\n",
            "Learning Rate: 6.07e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5400it [47:42,  1.76it/s, loss_1=8.2408, loss_2=8.3354, acc_1=0.0905, acc_2=0.0898, lr=6.01e-08, ppl_1=3792.61, ppl_2=4168.73]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 10800, Batch 5400/12000\n",
            "Loss 1: 8.7685, Accuracy: 0.0498\n",
            "Loss 2: 8.7329, Accuracy: 0.0000\n",
            "Learning Rate: 6.01e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5500it [48:36,  1.64it/s, loss_1=8.3407, loss_2=8.2462, acc_1=0.0771, acc_2=0.1024, lr=5.96e-08, ppl_1=4191.20, ppl_2=3813.13]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11000, Batch 5500/12000\n",
            "Loss 1: 8.7591, Accuracy: 0.0506\n",
            "Loss 2: 8.7233, Accuracy: 0.0000\n",
            "Learning Rate: 5.96e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5600it [49:29,  2.05it/s, loss_1=8.2492, loss_2=8.2499, acc_1=0.0935, acc_2=0.0981, lr=5.91e-08, ppl_1=3824.39, ppl_2=3827.33]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11200, Batch 5600/12000\n",
            "Loss 1: 8.7499, Accuracy: 0.0513\n",
            "Loss 2: 8.7137, Accuracy: 0.0000\n",
            "Learning Rate: 5.91e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5700it [50:22,  1.83it/s, loss_1=8.2717, loss_2=8.1661, acc_1=0.0978, acc_2=0.1115, lr=5.85e-08, ppl_1=3911.44, ppl_2=3519.58]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11400, Batch 5700/12000\n",
            "Loss 1: 8.7409, Accuracy: 0.0520\n",
            "Loss 2: 8.7043, Accuracy: 0.0000\n",
            "Learning Rate: 5.85e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5800it [51:15,  1.85it/s, loss_1=8.2497, loss_2=8.1638, acc_1=0.0925, acc_2=0.1020, lr=5.80e-08, ppl_1=3826.58, ppl_2=3511.48]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11600, Batch 5800/12000\n",
            "Loss 1: 8.7318, Accuracy: 0.0527\n",
            "Loss 2: 8.6953, Accuracy: 0.0000\n",
            "Learning Rate: 5.80e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5900it [52:09,  2.33it/s, loss_1=8.2667, loss_2=8.0460, acc_1=0.0894, acc_2=0.1250, lr=5.75e-08, ppl_1=3891.89, ppl_2=3121.36]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 11800, Batch 5900/12000\n",
            "Loss 1: 8.7231, Accuracy: 0.0534\n",
            "Loss 2: 8.6863, Accuracy: 0.0000\n",
            "Learning Rate: 5.75e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6000it [53:02,  1.52it/s, loss_1=8.2156, loss_2=8.2412, acc_1=0.0854, acc_2=0.0934, lr=5.71e-08, ppl_1=3698.18, ppl_2=3794.21]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 12000, Batch 6000/12000\n",
            "Loss 1: 8.7145, Accuracy: 0.0541\n",
            "Loss 2: 8.6774, Accuracy: 0.0000\n",
            "Learning Rate: 5.71e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6100it [53:54,  1.83it/s, loss_1=8.1526, loss_2=8.0922, acc_1=0.0926, acc_2=0.1152, lr=5.66e-08, ppl_1=3472.36, ppl_2=3268.71]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 12200, Batch 6100/12000\n",
            "Loss 1: 8.7062, Accuracy: 0.0547\n",
            "Loss 2: 8.6687, Accuracy: 0.0000\n",
            "Learning Rate: 5.66e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6200it [54:46,  1.92it/s, loss_1=8.1051, loss_2=8.1060, acc_1=0.0999, acc_2=0.1137, lr=5.61e-08, ppl_1=3311.20, ppl_2=3314.37]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 12400, Batch 6200/12000\n",
            "Loss 1: 8.6979, Accuracy: 0.0553\n",
            "Loss 2: 8.6602, Accuracy: 0.0000\n",
            "Learning Rate: 5.61e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6232it [55:03,  1.89it/s, loss_1=8.2332, loss_2=8.2157, acc_1=0.1000, acc_2=0.1029, lr=5.60e-08, ppl_1=3763.69, ppl_2=3698.47]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rValidating: 0it [00:00, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "Time: 4229.4s\n",
            "Training Metrics:\n",
            "  Loss 1: 4.5158, Loss 2: 8.6574\n",
            "  Accuracy 1: 0.0555, Accuracy 2: 0.0000\n",
            "Validation Metrics:\n",
            "  Loss 1: 8.1461, Loss 2: 8.0800\n",
            "  Accuracy 1: 0.0916, Accuracy 2: 0.1144\n",
            "  BLEU 1: 0.0000, BLEU 2: 0.0022\n",
            "  CER 1: 0.9240, CER 2: 1.4266\n",
            "  WER 1: 1.8609, WER 2: 2.7116\n",
            "New best model saved!\n",
            "\n",
            "============================================================\n",
            "Epoch 2/2\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100it [00:51,  1.76it/s, loss_1=8.1115, loss_2=8.1902, acc_1=0.0915, acc_2=0.1050, lr=5.55e-08, ppl_1=3332.61, ppl_2=3605.34]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 12664, Batch 100/12000\n",
            "Loss 1: 8.1746, Accuracy: 0.0914\n",
            "Loss 2: 8.1201, Accuracy: 0.0000\n",
            "Learning Rate: 5.55e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 200it [01:43,  1.61it/s, loss_1=8.2250, loss_2=8.0854, acc_1=0.0815, acc_2=0.1143, lr=5.51e-08, ppl_1=3733.01, ppl_2=3246.59]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 12864, Batch 200/12000\n",
            "Loss 1: 8.1716, Accuracy: 0.0920\n",
            "Loss 2: 8.1182, Accuracy: 0.0000\n",
            "Learning Rate: 5.51e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 300it [02:37,  1.73it/s, loss_1=8.1492, loss_2=8.1272, acc_1=0.0940, acc_2=0.1006, lr=5.47e-08, ppl_1=3460.51, ppl_2=3385.34]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 13064, Batch 300/12000\n",
            "Loss 1: 8.1691, Accuracy: 0.0921\n",
            "Loss 2: 8.1151, Accuracy: 0.0000\n",
            "Learning Rate: 5.47e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 400it [03:30,  1.95it/s, loss_1=8.1645, loss_2=8.0601, acc_1=0.0961, acc_2=0.1269, lr=5.43e-08, ppl_1=3513.97, ppl_2=3165.51]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 13264, Batch 400/12000\n",
            "Loss 1: 8.1665, Accuracy: 0.0919\n",
            "Loss 2: 8.1095, Accuracy: 0.0000\n",
            "Learning Rate: 5.43e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 500it [04:22,  2.12it/s, loss_1=8.0494, loss_2=8.0685, acc_1=0.1051, acc_2=0.1163, lr=5.39e-08, ppl_1=3131.82, ppl_2=3192.34]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 13464, Batch 500/12000\n",
            "Loss 1: 8.1622, Accuracy: 0.0919\n",
            "Loss 2: 8.1075, Accuracy: 0.0000\n",
            "Learning Rate: 5.39e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 600it [05:15,  1.66it/s, loss_1=8.1752, loss_2=8.0659, acc_1=0.0968, acc_2=0.1085, lr=5.35e-08, ppl_1=3551.85, ppl_2=3183.91]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 13664, Batch 600/12000\n",
            "Loss 1: 8.1574, Accuracy: 0.0917\n",
            "Loss 2: 8.1034, Accuracy: 0.0000\n",
            "Learning Rate: 5.35e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 700it [06:09,  1.83it/s, loss_1=8.0767, loss_2=8.0987, acc_1=0.0992, acc_2=0.1108, lr=5.31e-08, ppl_1=3218.62, ppl_2=3290.16]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 13864, Batch 700/12000\n",
            "Loss 1: 8.1547, Accuracy: 0.0917\n",
            "Loss 2: 8.0984, Accuracy: 0.0000\n",
            "Learning Rate: 5.31e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 800it [07:03,  1.88it/s, loss_1=8.1928, loss_2=8.0321, acc_1=0.0861, acc_2=0.1009, lr=5.27e-08, ppl_1=3614.80, ppl_2=3078.33]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 14064, Batch 800/12000\n",
            "Loss 1: 8.1510, Accuracy: 0.0917\n",
            "Loss 2: 8.0925, Accuracy: 0.0000\n",
            "Learning Rate: 5.27e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 900it [07:56,  2.17it/s, loss_1=8.1291, loss_2=8.0926, acc_1=0.0914, acc_2=0.1124, lr=5.23e-08, ppl_1=3391.59, ppl_2=3270.10]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 14264, Batch 900/12000\n",
            "Loss 1: 8.1473, Accuracy: 0.0918\n",
            "Loss 2: 8.0889, Accuracy: 0.0000\n",
            "Learning Rate: 5.23e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1000it [08:49,  1.95it/s, loss_1=8.1011, loss_2=8.0731, acc_1=0.0943, acc_2=0.1109, lr=5.20e-08, ppl_1=3297.99, ppl_2=3206.99]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 14464, Batch 1000/12000\n",
            "Loss 1: 8.1443, Accuracy: 0.0918\n",
            "Loss 2: 8.0851, Accuracy: 0.0000\n",
            "Learning Rate: 5.20e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1100it [09:41,  1.70it/s, loss_1=8.1212, loss_2=8.0770, acc_1=0.0877, acc_2=0.1106, lr=5.16e-08, ppl_1=3364.92, ppl_2=3219.66]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 14664, Batch 1100/12000\n",
            "Loss 1: 8.1393, Accuracy: 0.0919\n",
            "Loss 2: 8.0801, Accuracy: 0.0000\n",
            "Learning Rate: 5.16e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1200it [10:34,  2.05it/s, loss_1=8.0750, loss_2=7.9842, acc_1=0.0903, acc_2=0.1384, lr=5.13e-08, ppl_1=3213.14, ppl_2=2934.30]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 14864, Batch 1200/12000\n",
            "Loss 1: 8.1365, Accuracy: 0.0918\n",
            "Loss 2: 8.0768, Accuracy: 0.0000\n",
            "Learning Rate: 5.13e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1300it [11:28,  1.95it/s, loss_1=8.0293, loss_2=7.9505, acc_1=0.0974, acc_2=0.1368, lr=5.09e-08, ppl_1=3069.63, ppl_2=2836.94]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 15064, Batch 1300/12000\n",
            "Loss 1: 8.1327, Accuracy: 0.0918\n",
            "Loss 2: 8.0739, Accuracy: 0.0000\n",
            "Learning Rate: 5.09e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1400it [12:20,  1.88it/s, loss_1=8.0201, loss_2=8.1024, acc_1=0.0942, acc_2=0.1147, lr=5.06e-08, ppl_1=3041.36, ppl_2=3302.28]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 15264, Batch 1400/12000\n",
            "Loss 1: 8.1292, Accuracy: 0.0918\n",
            "Loss 2: 8.0708, Accuracy: 0.0000\n",
            "Learning Rate: 5.06e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1500it [13:13,  1.85it/s, loss_1=8.1015, loss_2=8.0331, acc_1=0.0955, acc_2=0.1239, lr=5.03e-08, ppl_1=3299.36, ppl_2=3081.29]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 15464, Batch 1500/12000\n",
            "Loss 1: 8.1258, Accuracy: 0.0918\n",
            "Loss 2: 8.0673, Accuracy: 0.0000\n",
            "Learning Rate: 5.03e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1600it [14:06,  1.93it/s, loss_1=8.1582, loss_2=8.0549, acc_1=0.0910, acc_2=0.1132, lr=4.99e-08, ppl_1=3491.86, ppl_2=3149.25]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 15664, Batch 1600/12000\n",
            "Loss 1: 8.1231, Accuracy: 0.0918\n",
            "Loss 2: 8.0645, Accuracy: 0.0000\n",
            "Learning Rate: 4.99e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1700it [14:58,  1.83it/s, loss_1=8.0372, loss_2=7.9407, acc_1=0.0998, acc_2=0.1305, lr=4.96e-08, ppl_1=3093.97, ppl_2=2809.24]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 15864, Batch 1700/12000\n",
            "Loss 1: 8.1198, Accuracy: 0.0918\n",
            "Loss 2: 8.0610, Accuracy: 0.0000\n",
            "Learning Rate: 4.96e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1800it [15:51,  1.95it/s, loss_1=8.0240, loss_2=7.8191, acc_1=0.1001, acc_2=0.1762, lr=4.93e-08, ppl_1=3053.36, ppl_2=2487.78]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 16064, Batch 1800/12000\n",
            "Loss 1: 8.1163, Accuracy: 0.0918\n",
            "Loss 2: 8.0580, Accuracy: 0.0000\n",
            "Learning Rate: 4.93e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 1900it [16:44,  2.11it/s, loss_1=8.0489, loss_2=7.8640, acc_1=0.0948, acc_2=0.1564, lr=4.90e-08, ppl_1=3130.26, ppl_2=2601.91]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 16264, Batch 1900/12000\n",
            "Loss 1: 8.1129, Accuracy: 0.0919\n",
            "Loss 2: 8.0541, Accuracy: 0.0000\n",
            "Learning Rate: 4.90e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2000it [17:36,  2.08it/s, loss_1=7.9662, loss_2=8.0069, acc_1=0.0887, acc_2=0.1233, lr=4.87e-08, ppl_1=2881.85, ppl_2=3001.68]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 16464, Batch 2000/12000\n",
            "Loss 1: 8.1099, Accuracy: 0.0919\n",
            "Loss 2: 8.0505, Accuracy: 0.0000\n",
            "Learning Rate: 4.87e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2100it [18:30,  1.97it/s, loss_1=8.0491, loss_2=8.0467, acc_1=0.0982, acc_2=0.1186, lr=4.84e-08, ppl_1=3130.87, ppl_2=3123.40]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 16664, Batch 2100/12000\n",
            "Loss 1: 8.1064, Accuracy: 0.0919\n",
            "Loss 2: 8.0469, Accuracy: 0.0000\n",
            "Learning Rate: 4.84e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2200it [19:22,  2.01it/s, loss_1=8.1693, loss_2=8.1338, acc_1=0.0811, acc_2=0.1110, lr=4.81e-08, ppl_1=3530.98, ppl_2=3407.74]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 16864, Batch 2200/12000\n",
            "Loss 1: 8.1033, Accuracy: 0.0919\n",
            "Loss 2: 8.0440, Accuracy: 0.0000\n",
            "Learning Rate: 4.81e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2300it [20:15,  1.71it/s, loss_1=7.9798, loss_2=8.0953, acc_1=0.0998, acc_2=0.1178, lr=4.78e-08, ppl_1=2921.28, ppl_2=3278.98]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 17064, Batch 2300/12000\n",
            "Loss 1: 8.1005, Accuracy: 0.0919\n",
            "Loss 2: 8.0407, Accuracy: 0.0000\n",
            "Learning Rate: 4.78e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2400it [21:06,  1.85it/s, loss_1=7.9745, loss_2=7.9131, acc_1=0.0964, acc_2=0.1433, lr=4.76e-08, ppl_1=2906.04, ppl_2=2732.86]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 17264, Batch 2400/12000\n",
            "Loss 1: 8.0972, Accuracy: 0.0919\n",
            "Loss 2: 8.0371, Accuracy: 0.0000\n",
            "Learning Rate: 4.76e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2500it [22:00,  1.60it/s, loss_1=7.9695, loss_2=7.9776, acc_1=0.0950, acc_2=0.1262, lr=4.73e-08, ppl_1=2891.40, ppl_2=2914.96]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 17464, Batch 2500/12000\n",
            "Loss 1: 8.0941, Accuracy: 0.0919\n",
            "Loss 2: 8.0343, Accuracy: 0.0000\n",
            "Learning Rate: 4.73e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2600it [22:54,  2.22it/s, loss_1=8.0077, loss_2=7.9772, acc_1=0.0867, acc_2=0.1335, lr=4.70e-08, ppl_1=3004.12, ppl_2=2913.66]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 17664, Batch 2600/12000\n",
            "Loss 1: 8.0912, Accuracy: 0.0919\n",
            "Loss 2: 8.0314, Accuracy: 0.0000\n",
            "Learning Rate: 4.70e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2700it [23:44,  2.25it/s, loss_1=8.0644, loss_2=7.8073, acc_1=0.0921, acc_2=0.1722, lr=4.68e-08, ppl_1=3179.20, ppl_2=2458.38]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 17864, Batch 2700/12000\n",
            "Loss 1: 8.0884, Accuracy: 0.0919\n",
            "Loss 2: 8.0276, Accuracy: 0.0000\n",
            "Learning Rate: 4.68e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2800it [24:37,  1.93it/s, loss_1=8.0455, loss_2=7.8995, acc_1=0.0862, acc_2=0.1609, lr=4.65e-08, ppl_1=3119.77, ppl_2=2695.87]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 18064, Batch 2800/12000\n",
            "Loss 1: 8.0857, Accuracy: 0.0919\n",
            "Loss 2: 8.0243, Accuracy: 0.0000\n",
            "Learning Rate: 4.65e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 2900it [25:31,  1.84it/s, loss_1=8.0670, loss_2=7.8195, acc_1=0.0846, acc_2=0.1651, lr=4.62e-08, ppl_1=3187.63, ppl_2=2488.67]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 18264, Batch 2900/12000\n",
            "Loss 1: 8.0833, Accuracy: 0.0919\n",
            "Loss 2: 8.0209, Accuracy: 0.0000\n",
            "Learning Rate: 4.62e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3000it [26:25,  1.70it/s, loss_1=7.9772, loss_2=7.9774, acc_1=0.0946, acc_2=0.1196, lr=4.60e-08, ppl_1=2913.75, ppl_2=2914.23]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 18464, Batch 3000/12000\n",
            "Loss 1: 8.0805, Accuracy: 0.0919\n",
            "Loss 2: 8.0179, Accuracy: 0.0000\n",
            "Learning Rate: 4.60e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3100it [27:18,  1.98it/s, loss_1=8.0479, loss_2=7.7560, acc_1=0.0921, acc_2=0.2062, lr=4.57e-08, ppl_1=3127.25, ppl_2=2335.50]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 18664, Batch 3100/12000\n",
            "Loss 1: 8.0777, Accuracy: 0.0919\n",
            "Loss 2: 8.0149, Accuracy: 0.0000\n",
            "Learning Rate: 4.57e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3200it [28:11,  1.86it/s, loss_1=7.8261, loss_2=7.7815, acc_1=0.1071, acc_2=0.1750, lr=4.55e-08, ppl_1=2505.10, ppl_2=2395.81]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 18864, Batch 3200/12000\n",
            "Loss 1: 8.0750, Accuracy: 0.0919\n",
            "Loss 2: 8.0115, Accuracy: 0.0000\n",
            "Learning Rate: 4.55e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3300it [29:06,  1.63it/s, loss_1=8.0077, loss_2=7.9187, acc_1=0.0867, acc_2=0.1337, lr=4.53e-08, ppl_1=3004.05, ppl_2=2748.31]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 19064, Batch 3300/12000\n",
            "Loss 1: 8.0721, Accuracy: 0.0919\n",
            "Loss 2: 8.0086, Accuracy: 0.0000\n",
            "Learning Rate: 4.53e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3400it [29:59,  1.82it/s, loss_1=7.9392, loss_2=8.0038, acc_1=0.0864, acc_2=0.1156, lr=4.50e-08, ppl_1=2805.04, ppl_2=2992.20]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 19264, Batch 3400/12000\n",
            "Loss 1: 8.0693, Accuracy: 0.0920\n",
            "Loss 2: 8.0053, Accuracy: 0.0000\n",
            "Learning Rate: 4.50e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3500it [30:52,  1.92it/s, loss_1=7.8802, loss_2=7.8980, acc_1=0.1057, acc_2=0.1442, lr=4.48e-08, ppl_1=2644.39, ppl_2=2691.94]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 19464, Batch 3500/12000\n",
            "Loss 1: 8.0665, Accuracy: 0.0920\n",
            "Loss 2: 8.0022, Accuracy: 0.0000\n",
            "Learning Rate: 4.48e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3600it [31:47,  1.70it/s, loss_1=8.0163, loss_2=7.9682, acc_1=0.0869, acc_2=0.1401, lr=4.46e-08, ppl_1=3029.90, ppl_2=2887.73]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 19664, Batch 3600/12000\n",
            "Loss 1: 8.0639, Accuracy: 0.0920\n",
            "Loss 2: 7.9997, Accuracy: 0.0000\n",
            "Learning Rate: 4.46e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3700it [32:42,  1.93it/s, loss_1=7.9231, loss_2=7.8869, acc_1=0.1073, acc_2=0.1777, lr=4.43e-08, ppl_1=2760.28, ppl_2=2662.10]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 19864, Batch 3700/12000\n",
            "Loss 1: 8.0607, Accuracy: 0.0920\n",
            "Loss 2: 7.9967, Accuracy: 0.0000\n",
            "Learning Rate: 4.43e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3800it [33:35,  1.83it/s, loss_1=7.9146, loss_2=7.9250, acc_1=0.0894, acc_2=0.1555, lr=4.41e-08, ppl_1=2736.94, ppl_2=2765.68]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 20064, Batch 3800/12000\n",
            "Loss 1: 8.0581, Accuracy: 0.0920\n",
            "Loss 2: 7.9942, Accuracy: 0.0000\n",
            "Learning Rate: 4.41e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 3900it [34:31,  1.68it/s, loss_1=8.0880, loss_2=7.7812, acc_1=0.0761, acc_2=0.1792, lr=4.39e-08, ppl_1=3255.07, ppl_2=2395.16]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 20264, Batch 3900/12000\n",
            "Loss 1: 8.0556, Accuracy: 0.0920\n",
            "Loss 2: 7.9914, Accuracy: 0.0000\n",
            "Learning Rate: 4.39e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4000it [35:24,  1.87it/s, loss_1=7.9433, loss_2=7.8636, acc_1=0.0961, acc_2=0.1822, lr=4.37e-08, ppl_1=2816.76, ppl_2=2600.84]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 20464, Batch 4000/12000\n",
            "Loss 1: 8.0534, Accuracy: 0.0920\n",
            "Loss 2: 7.9886, Accuracy: 0.0000\n",
            "Learning Rate: 4.37e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4100it [36:21,  1.98it/s, loss_1=7.9842, loss_2=7.9212, acc_1=0.0903, acc_2=0.1484, lr=4.35e-08, ppl_1=2934.09, ppl_2=2755.04]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 20664, Batch 4100/12000\n",
            "Loss 1: 8.0507, Accuracy: 0.0920\n",
            "Loss 2: 7.9861, Accuracy: 0.0000\n",
            "Learning Rate: 4.35e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4200it [37:14,  1.76it/s, loss_1=7.9470, loss_2=7.8127, acc_1=0.0895, acc_2=0.1824, lr=4.33e-08, ppl_1=2827.20, ppl_2=2471.68]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 20864, Batch 4200/12000\n",
            "Loss 1: 8.0483, Accuracy: 0.0919\n",
            "Loss 2: 7.9833, Accuracy: 0.0000\n",
            "Learning Rate: 4.33e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4300it [38:06,  1.96it/s, loss_1=7.8768, loss_2=7.7438, acc_1=0.0955, acc_2=0.1940, lr=4.31e-08, ppl_1=2635.45, ppl_2=2307.24]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 21064, Batch 4300/12000\n",
            "Loss 1: 8.0457, Accuracy: 0.0920\n",
            "Loss 2: 7.9806, Accuracy: 0.0000\n",
            "Learning Rate: 4.31e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4400it [39:00,  1.88it/s, loss_1=7.9292, loss_2=7.7164, acc_1=0.0970, acc_2=0.2102, lr=4.29e-08, ppl_1=2777.17, ppl_2=2244.81]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 21264, Batch 4400/12000\n",
            "Loss 1: 8.0434, Accuracy: 0.0919\n",
            "Loss 2: 7.9780, Accuracy: 0.0000\n",
            "Learning Rate: 4.29e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4500it [39:55,  1.93it/s, loss_1=8.0290, loss_2=7.8738, acc_1=0.0827, acc_2=0.1654, lr=4.27e-08, ppl_1=3068.67, ppl_2=2627.63]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 21464, Batch 4500/12000\n",
            "Loss 1: 8.0410, Accuracy: 0.0919\n",
            "Loss 2: 7.9753, Accuracy: 0.0000\n",
            "Learning Rate: 4.27e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4600it [40:47,  1.70it/s, loss_1=7.9219, loss_2=7.8071, acc_1=0.0882, acc_2=0.1846, lr=4.25e-08, ppl_1=2757.09, ppl_2=2458.08]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 21664, Batch 4600/12000\n",
            "Loss 1: 8.0384, Accuracy: 0.0919\n",
            "Loss 2: 7.9726, Accuracy: 0.0000\n",
            "Learning Rate: 4.25e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4700it [41:41,  1.67it/s, loss_1=7.9305, loss_2=7.8090, acc_1=0.0940, acc_2=0.1824, lr=4.23e-08, ppl_1=2780.84, ppl_2=2462.64]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 21864, Batch 4700/12000\n",
            "Loss 1: 8.0362, Accuracy: 0.0919\n",
            "Loss 2: 7.9699, Accuracy: 0.0000\n",
            "Learning Rate: 4.23e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4800it [42:35,  1.86it/s, loss_1=7.9563, loss_2=7.7836, acc_1=0.0889, acc_2=0.1922, lr=4.21e-08, ppl_1=2853.45, ppl_2=2400.94]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 22064, Batch 4800/12000\n",
            "Loss 1: 8.0340, Accuracy: 0.0919\n",
            "Loss 2: 7.9673, Accuracy: 0.0000\n",
            "Learning Rate: 4.21e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 4900it [43:30,  1.66it/s, loss_1=8.0125, loss_2=7.7646, acc_1=0.0805, acc_2=0.1965, lr=4.19e-08, ppl_1=3018.43, ppl_2=2355.66]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 22264, Batch 4900/12000\n",
            "Loss 1: 8.0319, Accuracy: 0.0919\n",
            "Loss 2: 7.9648, Accuracy: 0.0000\n",
            "Learning Rate: 4.19e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5000it [44:21,  2.15it/s, loss_1=8.0031, loss_2=7.6272, acc_1=0.0888, acc_2=0.2471, lr=4.17e-08, ppl_1=2990.29, ppl_2=2053.33]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 22464, Batch 5000/12000\n",
            "Loss 1: 8.0296, Accuracy: 0.0919\n",
            "Loss 2: 7.9619, Accuracy: 0.0000\n",
            "Learning Rate: 4.17e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5100it [45:14,  1.76it/s, loss_1=7.7536, loss_2=7.9867, acc_1=0.1021, acc_2=0.1453, lr=4.15e-08, ppl_1=2330.05, ppl_2=2941.64]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 22664, Batch 5100/12000\n",
            "Loss 1: 8.0270, Accuracy: 0.0919\n",
            "Loss 2: 7.9592, Accuracy: 0.0000\n",
            "Learning Rate: 4.15e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5200it [46:08,  1.65it/s, loss_1=7.9555, loss_2=7.9346, acc_1=0.0870, acc_2=0.1482, lr=4.13e-08, ppl_1=2851.33, ppl_2=2792.26]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 22864, Batch 5200/12000\n",
            "Loss 1: 8.0246, Accuracy: 0.0920\n",
            "Loss 2: 7.9566, Accuracy: 0.0000\n",
            "Learning Rate: 4.13e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5300it [47:01,  2.15it/s, loss_1=7.9134, loss_2=7.6976, acc_1=0.0968, acc_2=0.2325, lr=4.12e-08, ppl_1=2733.57, ppl_2=2202.95]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 23064, Batch 5300/12000\n",
            "Loss 1: 8.0223, Accuracy: 0.0919\n",
            "Loss 2: 7.9541, Accuracy: 0.0000\n",
            "Learning Rate: 4.12e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5400it [47:54,  2.06it/s, loss_1=7.8359, loss_2=7.7788, acc_1=0.0899, acc_2=0.1976, lr=4.10e-08, ppl_1=2529.89, ppl_2=2389.52]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 23264, Batch 5400/12000\n",
            "Loss 1: 8.0202, Accuracy: 0.0919\n",
            "Loss 2: 7.9516, Accuracy: 0.0000\n",
            "Learning Rate: 4.10e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5500it [48:46,  1.99it/s, loss_1=7.9729, loss_2=7.9225, acc_1=0.0877, acc_2=0.1486, lr=4.08e-08, ppl_1=2901.15, ppl_2=2758.63]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 23464, Batch 5500/12000\n",
            "Loss 1: 8.0181, Accuracy: 0.0919\n",
            "Loss 2: 7.9488, Accuracy: 0.0000\n",
            "Learning Rate: 4.08e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5600it [49:40,  1.95it/s, loss_1=7.9883, loss_2=7.7635, acc_1=0.0843, acc_2=0.1971, lr=4.06e-08, ppl_1=2946.17, ppl_2=2353.08]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 23664, Batch 5600/12000\n",
            "Loss 1: 8.0160, Accuracy: 0.0919\n",
            "Loss 2: 7.9464, Accuracy: 0.0000\n",
            "Learning Rate: 4.06e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5700it [50:34,  1.77it/s, loss_1=7.9666, loss_2=7.8682, acc_1=0.0875, acc_2=0.1539, lr=4.05e-08, ppl_1=2883.04, ppl_2=2612.92]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 23864, Batch 5700/12000\n",
            "Loss 1: 8.0139, Accuracy: 0.0919\n",
            "Loss 2: 7.9437, Accuracy: 0.0000\n",
            "Learning Rate: 4.05e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5800it [51:28,  1.67it/s, loss_1=7.8957, loss_2=7.7768, acc_1=0.0924, acc_2=0.1989, lr=4.03e-08, ppl_1=2685.80, ppl_2=2384.53]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 24064, Batch 5800/12000\n",
            "Loss 1: 8.0116, Accuracy: 0.0919\n",
            "Loss 2: 7.9410, Accuracy: 0.0000\n",
            "Learning Rate: 4.03e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 5900it [52:22,  1.74it/s, loss_1=7.9542, loss_2=7.8376, acc_1=0.0912, acc_2=0.1727, lr=4.01e-08, ppl_1=2847.51, ppl_2=2534.17]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 24264, Batch 5900/12000\n",
            "Loss 1: 8.0095, Accuracy: 0.0919\n",
            "Loss 2: 7.9384, Accuracy: 0.0000\n",
            "Learning Rate: 4.01e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6000it [53:15,  2.02it/s, loss_1=7.8121, loss_2=7.5892, acc_1=0.0931, acc_2=0.2549, lr=4.00e-08, ppl_1=2470.32, ppl_2=1976.79]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 24464, Batch 6000/12000\n",
            "Loss 1: 8.0072, Accuracy: 0.0919\n",
            "Loss 2: 7.9360, Accuracy: 0.0000\n",
            "Learning Rate: 4.00e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6100it [54:08,  1.62it/s, loss_1=7.9064, loss_2=7.8494, acc_1=0.0898, acc_2=0.1706, lr=3.98e-08, ppl_1=2714.71, ppl_2=2564.28]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 24664, Batch 6100/12000\n",
            "Loss 1: 8.0052, Accuracy: 0.0919\n",
            "Loss 2: 7.9336, Accuracy: 0.0000\n",
            "Learning Rate: 3.98e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6200it [55:00,  1.76it/s, loss_1=7.9596, loss_2=7.7910, acc_1=0.0850, acc_2=0.1936, lr=3.96e-08, ppl_1=2862.79, ppl_2=2418.71]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 24864, Batch 6200/12000\n",
            "Loss 1: 8.0030, Accuracy: 0.0919\n",
            "Loss 2: 7.9310, Accuracy: 0.0000\n",
            "Learning Rate: 3.96e-08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 6232it [55:17,  1.88it/s, loss_1=7.9054, loss_2=7.6917, acc_1=0.0920, acc_2=0.2372, lr=3.96e-08, ppl_1=2712.02, ppl_2=2190.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "Time: 4247.4s\n",
            "Training Metrics:\n",
            "  Loss 1: 4.1559, Loss 2: 7.9302\n",
            "  Accuracy 1: 0.0919, Accuracy 2: 0.0000\n",
            "Validation Metrics:\n",
            "  Loss 1: 7.8294, Loss 2: 7.7176\n",
            "  Accuracy 1: 0.0915, Accuracy 2: 0.1990\n",
            "  BLEU 1: 0.0000, BLEU 2: 0.0098\n",
            "  CER 1: 0.9095, CER 2: 1.4271\n",
            "  WER 1: 1.8240, WER 2: 2.6368\n",
            "New best model saved!\n",
            "\n",
            "Training completed!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VN_MuqaHxmjb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}